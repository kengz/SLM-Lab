import random
from collections import deque
import copy
import numpy as np
import pydash as ps
import torch
from torch.distributions.kl import kl_divergence

from slm_lab.agent import memory
from slm_lab.agent.agent import agent_util
from slm_lab.agent.algorithm import meta_algorithm
from slm_lab.agent.algorithm import policy_util
from slm_lab.lib import logger, util
from slm_lab.lib.decorator import lab_api
from slm_lab.lib.util import PID
from slm_lab.lib import math_util, util
from slm_lab.agent.net import net_util
from slm_lab.agent.algorithm.meta_algorithm.learning_equilibrium import LE, copy_weights_between_networks

logger = logger.get_logger(__name__)

# TODO make it inherit LE algo
class LEExploiter(meta_algorithm.LE):
    """
    Env must returns symmetrical agent states (giving agent 2 state to agent 1 should be fine)
    The base logging behavior is averaging over one episode.
    """

    # TODO add docs
    # TODO make it work for epi (how to divide epi when defection in during an epi ? (=> wait for end ?)

    def __init__(self, agent, global_nets, algorithm_spec,
                 memory_spec, net_spec, algo_idx=0):

        super().__init__(agent, global_nets, algorithm_spec, memory_spec, net_spec, algo_idx)

        self.punish_algo_idx = 2
        self.being_punished = False

    def act(self, state):
        other_ag_algorithms = agent_util.get_from_other_agents(self.agent, key="algorithm", default=[])
        if any(isinstance(opp_algo, LE) and opp_algo.last_used_algo == opp_algo.punish_algo_idx
               for opp_algo in other_ag_algorithms):

            self.being_punished = True
            self.active_algo_idx = self.punish_algo_idx
            self.remeaning_punishing_time = self.punishement_time
            print("self.being_punished", self.being_punished)

        action, action_pd = self.algorithms[self.active_algo_idx].act(state)

        if not self.being_punished and self.active_algo_idx == self.coop_algo_idx:
            if self.is_fully_init and self.n_steps_since_start >= self.length_of_history + self.warmup_length:
                    # next_defection_metric = self.defection_from_next_action(state, action)
                    with torch.no_grad():
                        next_defection_metric = [self.defection_from_next_action(state, action) for i in range(10)]
                    # print("self.defection_metric", self.defection_metric, "next_defection_metric", next_defection_metric)
                    # if next_defection_metric >= 0:
                    if any(m >= 0 for m in next_defection_metric):
                        print("defection_metric over 0")
                        action, action_pd = self.algorithms[self.punish_algo_idx].act(state)
                        self.active_algo_idx = self.punish_algo_idx
                        self.remeaning_punishing_time = self.punishement_time

        # To log action prob distrib
        if self.active_algo_idx == self.coop_algo_idx:
            self.action_pd_coop.append(action_pd.probs[0])
        elif self.active_algo_idx == self.punish_algo_idx:
            self.action_pd_punish.append(action_pd.probs[0])

        self.last_used_algo = self.active_algo_idx


        if self.debug:
            logger.info(f"action_pd {self.agent.agent_idx} {action_pd.probs}")

        return action, action_pd



    def defection_from_next_action(self, state, action):

        # Get the observed data
        # ag_action = [agent_util.get_from_current_agents(self.agent, key="action", default=None)]
        # ag_states = [agent_util.get_from_current_agents(self.agent, key="state", default=None)]
        ag_action = [np.asarray(action)]
        ag_states = [np.asarray(state)]
        ag_algorithms = [agent_util.get_from_current_agents(self.agent, key="algorithm", default=None)]
        # print("state",state, ag_states)
        # print("action",action, ag_action)

        # For each opponents (only tested for 1 opponent)
        for opp_idx, (s, a, algo) in enumerate(zip(ag_states, ag_action, ag_algorithms)):

            if self.opp_policy_from_supervised_learning:
                self.coop_net_simul_opponent_idx = 1 + 2 * opp_idx + 1
                self.approx_net_opponent_policy_idx = 1 + 2 * opp_idx + 2
            else:
                self.coop_net_simul_opponent_idx = 1 + opp_idx + 1

            self.data_queue_tmp = copy.deepcopy(self.data_queue)
            self.put_log_likelihood_in_data_buffer(algo, s, a, opp_idx, self.data_queue_tmp, log=False)

            # if self.n_steps_since_start >= self.length_of_history + self.warmup_length:
            percentile_value = self.compare_log_likelihood_on_sequence(opp_idx, self.data_queue_tmp, log=False)
            epi_defection_metric = -percentile_value
            # next_defection_metric = ((sum(self.defection_carac_queue) + epi_defection_metric)
            #                           / (len(self.defection_carac_queue) + 1 + self.epsilon))
            next_defection_metric = epi_defection_metric

        return next_defection_metric

    def defection_from_observed_actions(self, state, action, welfare, next_state, done):
        # TODO prob: this currently only works with 2 agents and with a discrete action space
        train = True

        # Get the observed data
        ag_action = [agent_util.get_from_current_agents(self.agent, key="action", default=None)]
        ag_states = [agent_util.get_from_current_agents(self.agent, key="state", default=None)]
        ag_rewards = [agent_util.get_from_current_agents(self.agent, key="reward", default=None)]
        ag_welfares = [agent_util.get_from_current_agents(self.agent, key="welfare", default=None)]
        ag_next_states = [agent_util.get_from_current_agents(self.agent, key="next_state", default=None)]
        ag_algorithms = [agent_util.get_from_current_agents(self.agent, key="algorithm", default=None)]
        other_ag_algorithms = [agent_util.get_from_other_agents(self.agent, key="algorithm", default=[])]

        # For him-self
        for self_index, (s, a, r, w, n_s, algo) in enumerate(zip(ag_states, ag_action,
                                                              ag_rewards, ag_welfares,
                                                              ag_next_states,
                                                              ag_algorithms)):

            if self.opp_policy_from_supervised_learning:
                self.coop_net_simul_opponent_idx = 2 # 1 + 2 * self_index + 1
                self.approx_net_opponent_policy_idx = 3 #1 + 2 * self_index + 2
            else:
                self.coop_net_simul_opponent_idx = 2 #1 + self_index + 1

            if not self.is_fully_init:
                self.data_queue.append(deque(maxlen=self.length_of_history))
                if self.same_init_weights:
                    logger.info("LE algo finishing init by copying weight between simu coop and opp approx")
                    copy_weights_between_networks(copy_from_net=self.algorithms[self.approx_net_opponent_policy_idx].net,
                                                  copy_to_net=self.algorithms[self.coop_net_simul_opponent_idx].net)

            # if not (isinstance(algo, LE) and algo.last_used_algo == algo.punish_algo_idx):
            # The opponent agent not is currenlty in the punish "state"

            self.n_steps_since_start += 1
            self.put_log_likelihood_in_data_buffer(algo, s, a, self_index, self.data_queue)

            self.ipm_memory_update(self_index, algo, s, a, r, n_s, done)
            self.apply_ipm_options_every_steps()

            if any(isinstance(opp_algo, LE) and opp_algo.last_used_algo == opp_algo.punish_algo_idx
                   for opp_algo in other_ag_algorithms):
                self.being_punished = True

            if done and self.use_strat_2:
                self.block_pos += 1
                if self.block_pos % self.block_len == 0:
                    self.train_simu_coop_from_scratch(self_index)

            if done and self.n_steps_since_start >= self.length_of_history + self.warmup_length:
                percentile_value = self.compare_log_likelihood_on_sequence(self_index, self.data_queue)
                self._update_defection_metric(epi_defection_metric=-percentile_value)

        if not self.is_fully_init:
            self.is_fully_init = True

        return train


    def ipm_memory_update(self, opp_idx, algo, s, a, r, n_s, done):
        # Update the coop networks simulating the opponents
        # computed_w = self.agent.welfare_function(algo.agent, r)
        computed_w = agent_util.utilitarian_welfare(algo.agent, r)
        self.last_computed_w = computed_w

        if self.opp_policy_from_supervised_learning:
            if self.use_historical_policy_as_target:
                self.opp_historical_policy = self.approximate_policy_from_history(opp_idx, separate_actions=False)

        if self.use_sl_for_simu_coop:
            self.algorithms[self.coop_net_simul_opponent_idx].memory_update(s, a, computed_w, n_s, done, idx=1)
        elif self.use_strat_2:
            self.memory.update(s, a, computed_w, n_s, done)
        else:
            self.algorithms[self.coop_net_simul_opponent_idx].memory_update(s, a, computed_w, n_s, done)

        if self.opp_policy_from_supervised_learning:

            # Update the networks learning the actual opponents policy (with supervised learning)
            if self.use_historical_policy_as_target:
                one_hot_target = self.opp_historical_policy[self.hash_fn(s)]
                # print("target",one_hot_target)
                self.algorithms[self.approx_net_opponent_policy_idx].memory_update(s, one_hot_target, None, None,
                                                                              done)
                if self.use_sl_for_simu_coop:
                    self.algorithms[self.coop_net_simul_opponent_idx].memory_update(s, one_hot_target, None, None,
                                                                               done, idx=0)
            else:
                self.algorithms[self.approx_net_opponent_policy_idx].memory_update(s, a, None, None, done)

    @lab_api
    def memory_update(self, state, action, welfare, next_state, done):

        # print("state, action, welfare, next_state, done", state, action, welfare, next_state, done)

        # start at the last step before the end of min_cooperative_epi_after_punishement
        # TODO remove train var always to True
        train_current_active_algo = self.detect_defection(state, action, welfare, next_state, done)

        assert (self.remeaning_punishing_time > 0) == (self.active_algo_idx == self.punish_algo_idx), (
                f"self.remeaning_punishing_time {self.remeaning_punishing_time} "
                f"self.active_algo_idx, self.coop_algo_idx {self.active_algo_idx} {self.coop_algo_idx}")
        assert (self.remeaning_punishing_time <= 0) == (self.active_algo_idx == self.coop_algo_idx)
        assert (self.active_algo_idx == self.punish_algo_idx) or (self.active_algo_idx == self.coop_algo_idx)


        outputs = None

        # Reset after a log
        if self.remeaning_punishing_time > 0:
            # agents_rewards = agent_util.get_from_current_agents(self.agent, key="reward", default=None)
            # welfare = agent_util.utilitarian_welfare(self.agent, agents_rewards)
            self.n_punishement_steps += 1

            if self.always_train_puni:
                if train_current_active_algo:  # This is currently always True
                    outputs = self.algorithms[self.coop_algo_idx].memory_update(state, action, welfare,
                                                                                next_state, done)
        else:
            self.n_cooperation_steps += 1

            if train_current_active_algo: # This is currently always True
                outputs = self.algorithms[self.coop_algo_idx].memory_update(state, action, welfare,
                                                                              next_state, done)

        if done:

            if self.remeaning_punishing_time <= - (self.min_cooperative_epi_after_punishement - 1):
                if self.defection_metric > self.defection_carac_threshold or self.being_punished:
                    self.detected_defection = True
            self.being_punished = False

            # Averaged by episode
            self.to_log["coop_frac"] = round(self.n_cooperation_steps /
                                            (self.n_punishement_steps + self.n_cooperation_steps), 2)
            self.n_cooperation_steps = 0
            self.n_punishement_steps = 0

            if self.remeaning_punishing_time > - self.min_cooperative_epi_after_punishement:
                self.remeaning_punishing_time -= 1

            # Switch from coop to punishement only at the start of epi
            if self.detected_defection:
                self.active_algo_idx = self.punish_algo_idx
                # self.active_algo_idx = self.coop_algo_idx
                self.remeaning_punishing_time = self.punishement_time
                self.detected_defection = False
                logger.debug("DEFECTION DETECTED")

            if self.remeaning_punishing_time <= 0:
                self.active_algo_idx = self.coop_algo_idx
                # self.active_algo_idx = self.punish_algo_idx
        return outputs
