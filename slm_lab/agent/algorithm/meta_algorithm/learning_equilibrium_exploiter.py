import random
from collections import deque
import copy
import numpy as np
import pydash as ps
import torch
from torch.distributions.kl import kl_divergence

from slm_lab.agent import memory
from slm_lab.agent.agent import agent_util
from slm_lab.agent.algorithm import meta_algorithm
from slm_lab.agent.algorithm import policy_util
from slm_lab.lib import logger, util
from slm_lab.lib.decorator import lab_api
from slm_lab.lib.util import PID
from slm_lab.lib import math_util, util
from slm_lab.agent.net import net_util
from slm_lab.agent.algorithm.meta_algorithm.learning_equilibrium import LE, copy_weights_between_networks

logger = logger.get_logger(__name__)

# TODO make it inherit LE algo
class LEExploiter(meta_algorithm.LE):
    """
    Env must returns symmetrical agent states (giving agent 2 state to agent 1 should be fine)
    The base logging behavior is averaging over one episode.
    """

    # TODO add docs
    # TODO make it work for epi (how to divide epi when defection in during an epi ? (=> wait for end ?)

    def __init__(self, agent, global_nets, algorithm_spec,
                 memory_spec, net_spec, algo_idx=0):

        super().__init__(agent, global_nets, algorithm_spec, memory_spec, net_spec, algo_idx)

        self.DOUBLE_COOP = True
        if self.DOUBLE_COOP:
            self.PUNISH_ALGO_IDX = 1
        else:
            self.PUNISH_ALGO_IDX = 2
        self.being_punished = False

    def act(self, state):
        other_ag_algorithms = agent_util.get_from_other_agents(self.agent, key="algorithm", default=[])
        if any(isinstance(opp_algo, LE) and opp_algo.last_used_algo == opp_algo.PUNISH_ALGO_IDX
               for opp_algo in other_ag_algorithms):

            self.being_punished = True
            self.active_algo_idx = self.PUNISH_ALGO_IDX
            self.remeaning_punishing_time = self.punishement_time
            print("self.being_punished", self.being_punished)

        action, action_pd = self.algorithms[self.active_algo_idx].act(state)

        if not self.being_punished and self.active_algo_idx == self.COOP_ALGO_IDX:
            if self.is_fully_init and self.n_steps_since_start >= self.length_of_history + self.WARMUP_LENGTH:
                    with torch.no_grad():
                        next_defection_metric = [self._defection_from_next_action(state, action) for _ in range(10)]
                        # print(f"next_defection_metric {next_defection_metric}")

                    if any(m >= 0 for m in next_defection_metric):
                        print("exploiter predicts defection_metric over 0")
                        # Playing the cooperative policy
                        action, action_pd = self.algorithms[self.PUNISH_ALGO_IDX].act(state)
                        self.active_algo_idx = self.PUNISH_ALGO_IDX
                        self.remeaning_punishing_time = self.punishement_time

                        # next_defection_metric, action, action_pd = self._defection_from_next_action(state, action)
                    # else:
                    #     print(f"exploiter selfish action action_pd {action_pd}")

        # To log action prob distrib
        if self.active_algo_idx == self.COOP_ALGO_IDX:
            self.action_pd_coop.append(action_pd.probs[0])
        elif self.active_algo_idx == self.PUNISH_ALGO_IDX:
            self.action_pd_punish.append(action_pd.probs[0])

        self.last_used_algo = self.active_algo_idx


        # if self.DEBUG:
        #     logger.info(f"action_pd {self.agent.agent_idx} {action_pd.probs}")
        # if self.DEBUG:
        # logger.info(f"agent {self.agent.agent_idx} active_algo {self.active_algo_idx} proba {action_pd.probs} "
        #             f"tau {self.algorithms[self.active_algo_idx].explore_var_scheduler.val}")

        return action, action_pd



    def _defection_from_next_action(self, state, action):

        # Get the observed data
        ag_action = [np.asarray(action)]
        ag_states = [np.asarray(state)]
        ag_algorithms = [agent_util.get_from_current_agents(self.agent, key="algorithm", default=None)]

        opp_idx, s, a, algo = 0, ag_states[0], ag_action[0], ag_algorithms[0]
        # For each opponents (only tested for 1 opponent)
        # for opp_idx, (s, a, algo) in enumerate(zip(ag_states, ag_action, ag_algorithms)):

        if self.opp_policy_from_supervised_learning:
            self.coop_net_simul_opponent_idx = 1 + 2 * opp_idx + 1
            self.approx_net_opponent_policy_idx = 1 + 2 * opp_idx + 2
        else:
            self.coop_net_simul_opponent_idx = 1 + opp_idx + 1

        self.data_queue_tmp = copy.deepcopy(self.data_queue)
        self._put_log_likelihood_in_data_buffer(algo, s, a, opp_idx, self.data_queue_tmp, log=False)

        percentile_value = self._compare_log_likelihood_on_boostrapped_sequences(opp_idx, self.data_queue_tmp, log=False,
                                                                                 last_step_is_mandatory=True)
        epi_defection_metric = -percentile_value
        next_defection_metric = epi_defection_metric

        return next_defection_metric

    def _defection_from_observed_actions(self, state, action, welfare, next_state, done):
        # TODO prob: this currently only works with 2 agents and with a discrete action space
        train = True

        # Get the observed data
        ag_action = [agent_util.get_from_current_agents(self.agent, key="action", default=None)]
        ag_states = [agent_util.get_from_current_agents(self.agent, key="state", default=None)]
        ag_rewards = [agent_util.get_from_current_agents(self.agent, key="reward", default=None)]
        ag_welfares = [agent_util.get_from_current_agents(self.agent, key="welfare", default=None)]
        ag_next_states = [agent_util.get_from_current_agents(self.agent, key="next_state", default=None)]
        ag_algorithms = [agent_util.get_from_current_agents(self.agent, key="algorithm", default=None)]
        other_ag_algorithms = [agent_util.get_from_other_agents(self.agent, key="algorithm", default=[])]

        # For him-self
        (self_index, s, a,
         r, w, n_s, algo) = (0, ag_states[0], ag_action[0],
                              ag_rewards[0], ag_welfares[0],
                              ag_next_states[0], ag_algorithms[0])

        # for self_index, (s, a, r, w, n_s, algo) in enumerate(zip(ag_states, ag_action,
        #                                                       ag_rewards, ag_welfares,
        #                                                       ag_next_states,
        #                                                       ag_algorithms)):

        if self.opp_policy_from_supervised_learning:
            self.coop_net_simul_opponent_idx = 2
            self.approx_net_opponent_policy_idx = 3
        else:
            self.coop_net_simul_opponent_idx = 2

        if not self.is_fully_init:
            self.data_queue.append(deque(maxlen=self.length_of_history))
            if self.same_init_weights:
                logger.info("LE algo finishing init by copying weight between simu coop and opp approx")
                copy_weights_between_networks(copy_from_net=self.algorithms[self.approx_net_opponent_policy_idx].net,
                                              copy_to_net=self.algorithms[self.coop_net_simul_opponent_idx].net)


        self.n_steps_since_start += 1
        self._put_log_likelihood_in_data_buffer(algo, s, a, self_index, self.data_queue)

        self._ipm_memory_update(self_index, algo, s, a, r, n_s, done)
        self._apply_ipm_options_every_steps()

        if any(isinstance(opp_algo, LE) and opp_algo.last_used_algo == opp_algo.PUNISH_ALGO_IDX
               for opp_algo in other_ag_algorithms):
            self.being_punished = True

        if done and self.use_strat_2:
            self.block_pos += 1
            if self.block_pos % self.block_len == 0:
                self._train_simu_coop_from_scratch(self_index)

        if done and self.n_steps_since_start >= self.length_of_history + self.WARMUP_LENGTH:
            percentile_value = self._compare_log_likelihood_on_boostrapped_sequences(self_index, self.data_queue)
            self._update_defection_metric(epi_defection_metric=-percentile_value)

        if not self.is_fully_init:
            self.is_fully_init = True

        return train


    def _ipm_memory_update(self, opp_idx, algo, s, a, r, n_s, done):
        # Update the coop networks simulating the opponents
        computed_w = agent_util.utilitarian_welfare(algo.agent, r)
        self.last_computed_w = computed_w

        if self.opp_policy_from_supervised_learning:
            if self.use_historical_policy_as_target:
                self.opp_historical_policy = self._approximate_policy_from_history(opp_idx, separate_actions=False)

        if self.use_strat_2:
            self.memory.update(s, a, computed_w, n_s, done)
        else:
            self.algorithms[self.coop_net_simul_opponent_idx].memory_update(s, a, computed_w, n_s, done)
        if self.DOUBLE_COOP:
            self.algorithms[self.PUNISH_ALGO_IDX].memory_update(s, a, computed_w, n_s, done)

        if self.opp_policy_from_supervised_learning:

            # Update the networks learning the actual opponents policy (with supervised learning)
            if self.use_historical_policy_as_target:
                one_hot_target = self.opp_historical_policy[self._hash_fn(s)]
                self.algorithms[self.approx_net_opponent_policy_idx].memory_update(s, one_hot_target, None, None,
                                                                              done)
            else:
                self.algorithms[self.approx_net_opponent_policy_idx].memory_update(s, a, None, None, done)

    @lab_api
    def memory_update(self, state, action, welfare, next_state, done):

        # start at the last step before the end of min_cooperative_epi_after_punishement
        # TODO remove train var always to True
        train_current_active_algo = self._detect_defection(state, action, welfare, next_state, done)

        assert (self.remeaning_punishing_time > 0) == (self.active_algo_idx == self.PUNISH_ALGO_IDX), (
                f"self.remeaning_punishing_time {self.remeaning_punishing_time} "
                f"self.active_algo_idx, self.coop_algo_idx {self.active_algo_idx} {self.COOP_ALGO_IDX}")
        assert (self.remeaning_punishing_time <= 0) == (self.active_algo_idx == self.COOP_ALGO_IDX)
        assert (self.active_algo_idx == self.PUNISH_ALGO_IDX) or (self.active_algo_idx == self.COOP_ALGO_IDX)


        outputs = None

        # Reset after a log
        if self.remeaning_punishing_time > 0:
            # agents_rewards = agent_util.get_from_current_agents(self.agent, key="reward", default=None)
            # welfare = agent_util.utilitarian_welfare(self.agent, agents_rewards)
            self.n_punishement_steps += 1

            if self.always_train_puni:
                if train_current_active_algo:  # This is currently always True
                    outputs = self.algorithms[self.COOP_ALGO_IDX].memory_update(state, action, welfare,
                                                                                next_state, done)
        else:
            self.n_cooperation_steps += 1

            if train_current_active_algo: # This is currently always True
                outputs = self.algorithms[self.COOP_ALGO_IDX].memory_update(state, action, welfare,
                                                                            next_state, done)

        if done:

            if self.remeaning_punishing_time <= - (self.min_cooperative_epi_after_punishement - 1):
                if self.defection_metric > self.defection_carac_threshold or self.being_punished:
                    self.detected_defection = True
            self.being_punished = False

            # Averaged by episode
            self.to_log["coop_frac"] = (self.n_cooperation_steps /
                                            (self.n_punishement_steps + self.n_cooperation_steps))
            self.n_cooperation_steps = 0
            self.n_punishement_steps = 0

            if self.remeaning_punishing_time > - self.min_cooperative_epi_after_punishement:
                self.remeaning_punishing_time -= 1

            # Switch from coop to punishement only at the start of epi
            if self.detected_defection:
                self.active_algo_idx = self.PUNISH_ALGO_IDX
                # self.active_algo_idx = self.coop_algo_idx
                self.remeaning_punishing_time = self.punishement_time
                self.detected_defection = False
                logger.debug("DEFECTION DETECTED")

            if self.remeaning_punishing_time <= 0:
                self.active_algo_idx = self.COOP_ALGO_IDX
                # self.active_algo_idx = self.punish_algo_idx
        return outputs















