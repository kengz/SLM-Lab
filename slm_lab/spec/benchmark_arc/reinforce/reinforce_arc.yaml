# REINFORCE Classic Control - TorchArcNet specs
# Covers: CartPole-v1 (multiple variants)

# -- Shared anchors --

_reinforce_meta: &reinforce_meta
  distributed: false
  eval_frequency: 500
  max_session: 4
  max_trial: 1
  log_frequency: 500

_reinforce_algorithm: &reinforce_algorithm
  name: Reinforce
  action_pdtype: default
  action_policy: default
  center_return: true
  explore_var_spec: null
  gamma: 0.99
  entropy_coef_spec: &entropy_linear_decay_001
    name: linear_decay
    start_val: 0.001
    end_val: 0.0
    start_step: 0
    end_step: 200000
  training_frequency: 512

_reinforce_net_128x2_tanh: &reinforce_net_128x2_tanh
  type: TorchArcNet
  arc:
    modules:
      body:
        Sequential:
          - LazyLinear: {out_features: 128}
          - Tanh:
          - LazyLinear: {out_features: 128}
          - Tanh:
    graph:
      input: x
      modules:
        body: [x]
      output: body
  hid_layers_activation: tanh
  init_fn: orthogonal_
  clip_grad_val: 0.5
  loss_spec:
    name: MSELoss
  optim_spec:
    name: Adam
    lr: 0.002

# -- Specs --

reinforce_cartpole_arc:
  agent:
    name: Reinforce
    algorithm:
      <<: *reinforce_algorithm
    memory:
      name: OnPolicyBatchReplay
    net:
      <<: *reinforce_net_128x2_tanh
  env:
    name: CartPole-v1
    num_envs: 4
    max_t: null
    max_frame: 200000
  meta:
    <<: *reinforce_meta

reinforce_baseline_cartpole_arc:
  agent:
    name: Reinforce
    algorithm:
      <<: *reinforce_algorithm
      entropy_coef_spec:
        name: linear_decay
        start_val: 0.01
        end_val: 0.001
        start_step: 0
        end_step: 20000
      training_frequency: 128
    memory:
      name: OnPolicyBatchReplay
    net:
      type: TorchArcNet
      arc:
        modules:
          body:
            Sequential:
              - LazyLinear: {out_features: 64}
              - SELU:
        graph:
          input: x
          modules:
            body: [x]
          output: body
      clip_grad_val: null
      loss_spec:
        name: MSELoss
      optim_spec:
        name: AdamW
        lr: 0.002
  env:
    name: CartPole-v1
    num_envs: 4
    max_t: null
    max_frame: 150000
  meta:
    <<: *reinforce_meta

reinforce_cartpole_extended_arc:
  agent:
    name: Reinforce
    algorithm:
      <<: *reinforce_algorithm
      gamma: 0.96
      entropy_coef_spec:
        name: linear_decay
        start_val: 0.005
        end_val: 0.0
        start_step: 0
        end_step: 250000
      training_frequency: 256
    memory:
      name: OnPolicyBatchReplay
    net:
      <<: *reinforce_net_128x2_tanh
      optim_spec:
        name: Adam
        lr: 0.0017
  env:
    name: CartPole-v1
    num_envs: 4
    max_t: null
    max_frame: 250000
  meta:
    <<: *reinforce_meta

reinforce_cartpole_boosted_arc:
  agent:
    name: Reinforce
    algorithm:
      <<: *reinforce_algorithm
      gamma: 0.965
      entropy_coef_spec:
        name: linear_decay
        start_val: 0.006
        end_val: 0.0
        start_step: 0
        end_step: 150000
      training_frequency: 256
    memory:
      name: OnPolicyBatchReplay
    net:
      type: TorchArcNet
      arc:
        modules:
          body:
            Sequential:
              - LazyLinear: {out_features: 256}
              - Tanh:
              - LazyLinear: {out_features: 256}
              - Tanh:
        graph:
          input: x
          modules:
            body: [x]
          output: body
      hid_layers_activation: tanh
      init_fn: orthogonal_
      clip_grad_val: 0.5
      loss_spec:
        name: MSELoss
      optim_spec:
        name: Adam
        lr: 0.0015
  env:
    name: CartPole-v1
    num_envs: 4
    max_t: null
    max_frame: 200000
  meta:
    <<: *reinforce_meta

reinforce_cartpole_asha_best_arc:
  agent:
    name: Reinforce
    algorithm:
      <<: *reinforce_algorithm
      gamma: 0.9791
      entropy_coef_spec:
        name: linear_decay
        start_val: 0.000658
        end_val: 0.0
        start_step: 0
        end_step: 200000
      training_frequency: 512
    memory:
      name: OnPolicyBatchReplay
    net:
      <<: *reinforce_net_128x2_tanh
      optim_spec:
        name: Adam
        lr: 0.00196
  env:
    name: CartPole-v1
    num_envs: 4
    max_t: null
    max_frame: 200000
  meta:
    <<: *reinforce_meta
    max_session: 8

reinforce_cartpole_mega_search_arc:
  agent:
    name: Reinforce
    algorithm:
      <<: *reinforce_algorithm
      gamma: 0.97
      entropy_coef_spec:
        name: linear_decay
        start_val: 0.003
        end_val: 0.0
        start_step: 0
        end_step: 120000
      training_frequency: 256
    memory:
      name: OnPolicyBatchReplay
    net:
      type: TorchArcNet
      arc:
        modules:
          body:
            Sequential:
              - LazyLinear: {out_features: 256}
              - Tanh:
              - LazyLinear: {out_features: 256}
              - Tanh:
        graph:
          input: x
          modules:
            body: [x]
          output: body
      hid_layers_activation: tanh
      init_fn: orthogonal_
      clip_grad_val: 0.5
      loss_spec:
        name: MSELoss
      optim_spec:
        name: Adam
        lr: 0.002
  env:
    name: CartPole-v1
    num_envs: 4
    max_t: null
    max_frame: 200000
  meta:
    <<: *reinforce_meta
    log_frequency: 2000
    max_trial: 12
  search:
    agent.algorithm.gamma__choice: [0.96, 0.97, 0.98]
    agent.algorithm.entropy_coef_spec.start_val__choice: [0.002, 0.003, 0.004]
    agent.net.optim_spec.lr__choice: [0.0015, 0.002, 0.0025]
