# A2C Classic Control - TorchArcNet specs
# Covers: CartPole-v1, Acrobot-v1, Pendulum-v1, LunarLander-v3 (discrete + continuous)

# -- Shared anchors --

_a2c_classic_meta: &a2c_classic_meta
  distributed: false
  log_frequency: 500
  eval_frequency: 1000
  max_session: 4
  max_trial: 1

_a2c_classic_net: &a2c_classic_net
  type: TorchArcNet
  shared: false
  arc:
    modules:
      body:
        Sequential:
          - LazyLinear: {out_features: 64}
          - Tanh:
          - LazyLinear: {out_features: 64}
          - Tanh:
    graph:
      input: x
      modules:
        body: [x]
      output: body
  hid_layers_activation: tanh
  init_fn: orthogonal_
  actor_init_std: 0.01
  critic_init_std: 1.0
  batch_norm: false
  clip_grad_val: 0.5
  use_same_optim: false
  loss_spec:
    name: MSELoss
  gpu: auto

_a2c_classic_algorithm: &a2c_classic_algorithm
  name: ActorCritic
  action_pdtype: default
  action_policy: default
  explore_var_spec: null
  gamma: 0.99
  lam: 0.95
  num_step_returns: null
  entropy_coef_spec: &entropy_linear_001
    name: linear_decay
    start_val: 0.01
    end_val: 0.0
    start_step: 0
    end_step: 200000
  val_loss_coef: 0.5
  training_frequency: 32

# -- Specs --

a2c_gae_cartpole_arc:
  agent:
    name: A2C
    algorithm:
      <<: *a2c_classic_algorithm
    memory:
      name: OnPolicyBatchReplay
    net:
      <<: *a2c_classic_net
      actor_optim_spec:
        name: Adam
        lr: 0.001
      critic_optim_spec:
        name: Adam
        lr: 0.003
  env:
    name: CartPole-v1
    num_envs: 4
    max_t: null
    max_frame: 200000
  meta:
    <<: *a2c_classic_meta

a2c_gae_acrobot_arc:
  agent:
    name: A2C
    algorithm:
      <<: *a2c_classic_algorithm
      entropy_coef_spec:
        name: linear_decay
        start_val: 0.01
        end_val: 0.0
        start_step: 0
        end_step: 300000
    memory:
      name: OnPolicyBatchReplay
    net:
      <<: *a2c_classic_net
      actor_optim_spec:
        name: Adam
        lr: 0.001
      critic_optim_spec:
        name: Adam
        lr: 0.003
  env:
    name: Acrobot-v1
    num_envs: 4
    max_t: null
    max_frame: 300000
  meta:
    <<: *a2c_classic_meta
    eval_frequency: 5000

a2c_gae_pendulum_arc:
  agent:
    name: A2C
    algorithm:
      <<: *a2c_classic_algorithm
      entropy_coef_spec:
        name: no_decay
        start_val: 0.0
        end_val: 0.0
        start_step: 0
        end_step: 0
      training_frequency: 2048
      normalize_v_targets: true
    memory:
      name: OnPolicyBatchReplay
    net:
      type: TorchArcNet
      shared: false
      arc:
        modules:
          body:
            Sequential:
              - LazyLinear: {out_features: 256}
              - ReLU:
              - LazyLinear: {out_features: 256}
              - ReLU:
        graph:
          input: x
          modules:
            body: [x]
          output: body
      hid_layers_activation: relu
      init_fn: orthogonal_
      actor_init_std: 0.01
      critic_init_std: 1.0
      batch_norm: false
      clip_grad_val: 0.5
      use_same_optim: true
      loss_spec:
        name: MSELoss
      optim_spec:
        name: Adam
        lr: 2.0e-3
      gpu: auto
  env:
    name: Pendulum-v1
    num_envs: 4
    max_t: null
    max_frame: 300000
    normalize_obs: true
  meta:
    <<: *a2c_classic_meta

a2c_gae_lunar_arc:
  agent:
    name: A2C
    algorithm:
      <<: *a2c_classic_algorithm
      gamma: 0.995
      entropy_coef_spec:
        name: no_decay
        start_val: 0.00001
        end_val: 0.00001
        start_step: 0
        end_step: 0
      training_frequency: 5
    memory:
      name: OnPolicyBatchReplay
    net:
      type: TorchArcNet
      shared: false
      arc:
        modules:
          body:
            Sequential:
              - LazyLinear: {out_features: 128}
              - Tanh:
              - LazyLinear: {out_features: 128}
              - Tanh:
        graph:
          input: x
          modules:
            body: [x]
          output: body
      hid_layers_activation: tanh
      init_fn: orthogonal_
      actor_init_std: 0.01
      critic_init_std: 1.0
      batch_norm: false
      clip_grad_val: 0.5
      use_same_optim: false
      loss_spec:
        name: MSELoss
      actor_optim_spec:
        name: Adam
        lr: 0.00083
      critic_optim_spec:
        name: Adam
        lr: 0.00083
      lr_scheduler_spec:
        name: LinearToZero
        frame: 300000
      gpu: auto
  env:
    name: LunarLander-v3
    num_envs: 8
    max_t: null
    max_frame: 300000
  meta:
    <<: *a2c_classic_meta
    log_frequency: 1000
    eval_frequency: 5000
    max_trial: 8
    search_resources:
      cpu: 1
      gpu: 0.125
    search_scheduler:
      grace_period: 50000
      reduction_factor: 3
  search:
    agent.algorithm.gamma__uniform: [0.99, 0.999]
    agent.algorithm.lam__uniform: [0.92, 0.98]
    agent.algorithm.training_frequency__qrandint: [5, 16, 1]
    agent.algorithm.entropy_coef_spec.start_val__loguniform: [0.000001, 0.0001]
    agent.net.actor_optim_spec.lr__loguniform: [0.0003, 0.002]
    agent.net.critic_optim_spec.lr__loguniform: [0.0003, 0.002]

a2c_gae_lunar_continuous_arc:
  agent:
    name: A2C
    algorithm:
      <<: *a2c_classic_algorithm
      entropy_coef_spec:
        name: no_decay
        start_val: 0.01
        end_val: 0.01
        start_step: 0
        end_step: 0
      training_frequency: 64
    memory:
      name: OnPolicyBatchReplay
    net:
      type: TorchArcNet
      shared: false
      arc:
        modules:
          body:
            Sequential:
              - LazyLinear: {out_features: 128}
              - Tanh:
              - LazyLinear: {out_features: 128}
              - Tanh:
        graph:
          input: x
          modules:
            body: [x]
          output: body
      hid_layers_activation: tanh
      init_fn: orthogonal_
      actor_init_std: 0.01
      critic_init_std: 1.0
      batch_norm: false
      clip_grad_val: 0.5
      use_same_optim: false
      loss_spec:
        name: MSELoss
      actor_optim_spec:
        name: Adam
        lr: 0.0007
      critic_optim_spec:
        name: Adam
        lr: 0.001
      lr_scheduler_spec:
        name: LinearToZero
        frame: 300000
      gpu: auto
  env:
    name: LunarLander-v3
    continuous: true
    num_envs: 8
    max_t: null
    max_frame: 300000
  meta:
    <<: *a2c_classic_meta
    log_frequency: 1000
    eval_frequency: 5000
    max_trial: 8
    search_resources:
      cpu: 1
      gpu: 0.125
    search_scheduler:
      grace_period: 50000
      reduction_factor: 3
  search:
    agent.algorithm.gamma__uniform: [0.98, 0.999]
    agent.algorithm.lam__uniform: [0.92, 0.98]
    agent.algorithm.training_frequency__qrandint: [32, 128, 16]
    agent.algorithm.entropy_coef_spec.start_val__loguniform: [0.005, 0.02]
    agent.net.actor_optim_spec.lr__loguniform: [0.0003, 0.003]
    agent.net.critic_optim_spec.lr__loguniform: [0.0005, 0.003]
