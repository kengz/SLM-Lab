# SAC Classic Control using TorchArcNet
# Covers: CartPole-v1, Acrobot-v1 (discrete), Pendulum-v1 (continuous)

# --- Shared anchors ---

_arc_body: &arc_body
  modules:
    body:
      Sequential:
        - LazyLinear:
            out_features: 256
        - ReLU:
        - LazyLinear:
            out_features: 256
        - ReLU:
  graph:
    input: x
    modules:
      body: [x]
    output: body

_sac_net: &sac_net
  type: TorchArcNet
  arc: *arc_body
  hid_layers_activation: relu
  clip_grad_val: null
  loss_spec:
    name: MSELoss
  optim_spec:
    name: Adam
    lr: 0.0003
  update_type: polyak
  polyak_coef: 0.005
  gpu: auto

_discrete_algorithm: &discrete_algorithm
  name: SoftActorCritic
  action_pdtype: Categorical
  action_policy: default
  gamma: 0.99
  training_frequency: 1
  training_start_step: 1000

_memory: &memory
  name: Replay
  batch_size: 256
  max_size: 100000
  use_cer: false

_meta: &meta
  distributed: false
  log_frequency: 500
  eval_frequency: 500
  max_session: 4
  max_trial: 1

# --- Specs ---

sac_cartpole_arc:
  agent:
    name: SoftActorCritic
    algorithm: *discrete_algorithm
    memory: *memory
    net: *sac_net
  env:
    name: CartPole-v1
    num_envs: 4
    max_t: null
    max_frame: 200000
  meta: *meta

sac_acrobot_arc:
  agent:
    name: SoftActorCritic
    algorithm: *discrete_algorithm
    memory: *memory
    net:
      <<: *sac_net
      clip_grad_val: 0.5
      loss_spec:
        name: SmoothL1Loss
  env:
    name: Acrobot-v1
    num_envs: 4
    max_t: null
    max_frame: 300000
  meta:
    <<: *meta
    eval_frequency: 5000

sac_pendulum_arc:
  agent:
    name: SoftActorCritic
    algorithm:
      name: SoftActorCritic
      action_pdtype: Normal
      action_policy: default
      gamma: 0.99
      training_iter: 4
      training_frequency: 1
      training_start_step: 256
    memory: *memory
    net: *sac_net
  env:
    name: Pendulum-v1
    num_envs: 4
    max_t: null
    max_frame: 300000
  meta:
    <<: *meta
    eval_frequency: 5000
