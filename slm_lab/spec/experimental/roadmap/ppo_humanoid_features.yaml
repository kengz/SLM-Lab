# PPO Humanoid feature ablation specs (showcase â€” features help here)

_ppo_base_algorithm: &ppo_base_algorithm
  name: PPO
  action_pdtype: default
  action_policy: default
  gamma: 0.997
  lam: 0.97
  clip_eps_spec:
    name: no_decay
    start_val: 0.2
    end_val: 0.2
    start_step: 0
    end_step: 0
  entropy_coef_spec:
    name: no_decay
    start_val: 0.001
    end_val: 0.001
    start_step: 0
    end_step: 0
  val_loss_coef: 0.5
  time_horizon: 512
  minibatch_size: 64
  training_epoch: 10
  normalize_v_targets: true

_ppo_base_net: &ppo_base_net
  type: MLPNet
  shared: false
  hid_layers: [256, 256]
  hid_layers_activation: tanh
  init_fn: orthogonal_
  normalize: true
  clip_grad_val: 0.5
  use_same_optim: true
  log_std_init: 0.0
  loss_spec:
    name: MSELoss
  optim_spec:
    name: AdamW
    lr: 0.0002
  gpu: auto

_ppo_base_meta: &ppo_base_meta
  distributed: false
  log_frequency: 5000
  eval_frequency: 5000
  max_session: 4
  max_trial: 1

# 1. Baseline
ppo_humanoid_baseline:
  agent:
    name: PPO
    algorithm: *ppo_base_algorithm
    memory:
      name: OnPolicyBatchReplay
    net: *ppo_base_net
  env:
    name: Humanoid-v5
    num_envs: 4
    max_t: null
    max_frame: 200000
    normalize_obs: true
    normalize_reward: true
  meta: *ppo_base_meta

# 2. With layer_norm
ppo_humanoid_layernorm:
  agent:
    name: PPO
    algorithm: *ppo_base_algorithm
    memory:
      name: OnPolicyBatchReplay
    net:
      <<: *ppo_base_net
      layer_norm: true
  env:
    name: Humanoid-v5
    num_envs: 4
    max_t: null
    max_frame: 200000
    normalize_obs: true
    normalize_reward: true
  meta: *ppo_base_meta

# 3. With percentile normalization
ppo_humanoid_percentile:
  agent:
    name: PPO
    algorithm:
      <<: *ppo_base_algorithm
      normalize_advantages: percentile
    memory:
      name: OnPolicyBatchReplay
    net: *ppo_base_net
  env:
    name: Humanoid-v5
    num_envs: 4
    max_t: null
    max_frame: 200000
    normalize_obs: true
    normalize_reward: true
  meta: *ppo_base_meta

# 4. v2 stack (layer_norm + percentile)
ppo_humanoid_v2:
  agent:
    name: PPO
    algorithm:
      <<: *ppo_base_algorithm
      normalize_advantages: percentile
    memory:
      name: OnPolicyBatchReplay
    net:
      <<: *ppo_base_net
      layer_norm: true
  env:
    name: Humanoid-v5
    num_envs: 4
    max_t: null
    max_frame: 200000
    normalize_obs: true
    normalize_reward: true
  meta: *ppo_base_meta
