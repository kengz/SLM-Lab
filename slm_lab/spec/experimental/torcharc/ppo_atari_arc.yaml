# PPO Atari using TorchArcNet - equivalent to ppo_atari ConvNet spec.
# Uses LazyConv2d/LazyLinear so input dims are inferred at first forward pass.
# Uses normalize=true in net spec to normalize pixel inputs to [0,1] on GPU in forward pass.

ppo_atari_arc:
  agent:
    name: PPO
    algorithm:
      name: PPO
      action_pdtype: default
      action_policy: default
      explore_var_spec: null
      gamma: 0.99
      lam: 0.95
      clip_eps_spec:
        name: no_decay
        start_val: 0.1
        end_val: 0.1
        start_step: 0
        end_step: 0
      entropy_coef_spec:
        name: no_decay
        start_val: 0.01
        end_val: 0.01
        start_step: 0
        end_step: 0
      val_loss_coef: 0.5
      time_horizon: 128
      minibatch_size: 256
      training_epoch: 4
      clip_vloss: true
    memory:
      name: OnPolicyBatchReplay
    net:
      type: TorchArcNet
      arc:
        modules:
          body:
            Sequential:
              - LazyConv2d:
                  out_channels: 32
                  kernel_size: 8
                  stride: 4
              - ReLU:
              - LazyConv2d:
                  out_channels: 64
                  kernel_size: 4
                  stride: 2
              - ReLU:
              - LazyConv2d:
                  out_channels: 64
                  kernel_size: 3
                  stride: 1
              - ReLU:
              - Flatten:
              - LazyLinear:
                  out_features: 512
              - ReLU:
        graph:
          input: x
          modules:
            body: [x]
          output: body
      shared: true
      hid_layers_activation: relu
      init_fn: orthogonal_
      clip_grad_val: 0.5
      use_same_optim: true
      loss_spec:
        name: MSELoss
      optim_spec:
        name: AdamW
        lr: 2.5e-4
        eps: 1e-5
      lr_scheduler_spec:
        name: LinearToZero
        frame: 10e6
      actor_init_std: 0.01
      critic_init_std: 1.0
      normalize: true
      gpu: auto
  env:
    name: ${env}
    num_envs: 16
    max_frame: 1e7
    life_loss_info: true
  meta:
    distributed: false
    eval_frequency: 10000
    log_frequency: 10000
    max_session: 4
    max_trial: 1
