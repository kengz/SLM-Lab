{
  "ipd_base_rf":{
    "world":{
      "name":"DefaultMultiAgentWorld"
    },
    "agent":[
      {
        "name":"Reinforce",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "welfare_function":"default_welfare",
        "algorithm":{
          "name":"Reinforce",
          "action_pdtype":"default",
          "action_policy":"default",
          "explore_var_spec":null,
          "gamma":0.0,
          "normalize_inputs":true,
          "center_return":true,
          "normalize_return":true,
          "normalize_over_n_batch":10,
          "entropy_coef_spec":{
            "name":"linear_decay",
            "start_val":2.0,
            "end_val":0.10,
            "start_step":0,
            "end_step":500
          },
          "training_frequency":1
        },
        "memory":{
          "name":"OnPolicyReplay"
        },
        "net":{
          "type":"MLPNet",
          "hid_layers":[1],
          "hid_layers_activation":"LeakyReLU",
          "init_fn":"normal_mu_0_std_0.1",
          "clip_grad_val":10.0,
          "optim_spec":{
            "name":"SGD",
            "lr":0.025,
            "momentum":0.75
          },
          "lr_scheduler_spec":{
            "name":"LinearToZero",
            "frame":5000
          }
        }
      },
      {
        "copy_n":0
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":5000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_trial":1
    }
  },
  "ipd_base_rf_util":{
    "world":{
      "name":"DefaultMultiAgentWorld"
    },
    "agent":[
      {
        "name":"Reinforce",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "welfare_function":"utilitarian_welfare",
        "algorithm":{
          "name":"Reinforce",
          "action_pdtype":"default",
          "action_policy":"default",
          "explore_var_spec":null,
          "gamma":0.0,
          "normalize_inputs":true,
          "center_return":true,
          "normalize_return":true,
          "normalize_over_n_batch":10,
          "entropy_coef_spec":{
            "name":"linear_decay",
            "start_val":2.0,
            "end_val":0.10,
            "start_step":0,
            "end_step":500
          },
          "training_frequency":1
        },
        "memory":{
          "name":"OnPolicyReplay"
        },
        "net":{
          "type":"MLPNet",
          "hid_layers":[1],
          "hid_layers_activation":"LeakyReLU",
          "init_fn":"normal_mu_0_std_0.1",
          "clip_grad_val":10.0,
          "optim_spec":{
            "name":"SGD",
            "lr":0.025,
            "momentum":0.75
          },
          "lr_scheduler_spec":{
            "name":"LinearToZero",
            "frame":5000
          }
        }
      },
      {
        "copy_n":0
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":5000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_trial":1
    }
  },
  "ipd_ppm_le_self_play":{
    "world":{
      "name":"DefaultMultiAgentWorld"
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "n_steps_in_bstrap_replts":50,
          "n_bootstrapped_replications":200,
          "length_of_history":200,
          "min_coop_time":0,
          "defection_detection_mode":"network_weights",
          "defection_carac_threshold":0.01,
          "average_d_carac":true,
          "average_d_carac_len":1,
          "same_init_weights":true,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":1,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"CrossEntropyLoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "normalize_inputs":true,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":2.0,
                  "end_val":0.10,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":10.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            }
          ]
        }
      },
      {
        "copy_n":0
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":5000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_trial":1
    }
  },
  "ipd_ppm_le_vs_naive_opp":{
    "world":{
      "name":"DefaultMultiAgentWorld"
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "n_steps_in_bstrap_replts":50,
          "n_bootstrapped_replications":200,
          "length_of_history":200,
          "min_coop_time":0,
          "defection_detection_mode":"network_weights",
          "defection_carac_threshold":0.01,
          "average_d_carac":true,
          "average_d_carac_len":1,
          "same_init_weights":true,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":1,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"CrossEntropyLoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "normalize_inputs":true,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":2.0,
                  "end_val":0.10,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":10.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            }
          ]
        }
      },
      {
        "name":"Reinforce",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"Reinforce",
          "action_pdtype":"default",
          "action_policy":"default",
          "explore_var_spec":null,
          "gamma":0.0,
          "normalize_inputs":true,
          "center_return":true,
          "normalize_return":true,
          "normalize_over_n_batch":10,
          "entropy_coef_spec":{
            "name":"linear_decay",
            "start_val":2.0,
            "end_val":0.10,
            "start_step":0,
            "end_step":500
          },
          "training_frequency":1
        },
        "memory":{
          "name":"OnPolicyReplay"
        },
        "net":{
          "type":"MLPNet",
          "hid_layers":[1],
          "hid_layers_activation":"LeakyReLU",
          "init_fn":"normal_mu_0_std_0.1",
          "clip_grad_val":10.0,
          "optim_spec":{
            "name":"SGD",
            "lr":0.025,
            "momentum":0.75
          },
          "lr_scheduler_spec":{
            "name":"LinearToZero",
            "frame":5000
          }
        }
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":5000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_trial":1
    }
  },

  "ipd_ipm_le_self_play_no_strat_lr_008":{
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "n_steps_in_bstrap_replts":50,
          "n_bootstrapped_replications":200,
          "length_of_history":200,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":1,
          "same_init_weights":true,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":1,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"CrossEntropyLoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "normalize_inputs":true,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":2.0,
                  "end_val":0.10,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":10.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1,
                "normalize_inputs":true
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"CrossEntropyLoss",
                  "reduction":"none"
                },
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            }
          ]
        }
      },
      {
        "copy_n":0
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":5000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_no_strat_lr_008":{
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "n_steps_in_bstrap_replts":50,
          "n_bootstrapped_replications":200,
          "length_of_history":200,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":1,
          "same_init_weights":true,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":1,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"CrossEntropyLoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "normalize_inputs":true,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":2.0,
                  "end_val":0.10,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":10.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1,
                "normalize_inputs":true
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"CrossEntropyLoss",
                  "reduction":"none"
                },
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            }
          ]
        }
      },
      {
        "name":"Reinforce",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"Reinforce",
          "action_pdtype":"default",
          "action_policy":"default",
          "explore_var_spec":null,
          "gamma":0.0,
          "normalize_inputs":true,
          "center_return":true,
          "normalize_return":true,
          "normalize_over_n_batch":10,
          "entropy_coef_spec":{
            "name":"linear_decay",
            "start_val":2.0,
            "end_val":0.10,
            "start_step":0,
            "end_step":500
          },
          "training_frequency":1
        },
        "memory":{
          "name":"OnPolicyReplay"
        },
        "net":{
          "type":"MLPNet",
          "hid_layers":[1],
          "hid_layers_activation":"LeakyReLU",
          "init_fn":"normal_mu_0_std_0.1",
          "clip_grad_val":10.0,
          "optim_spec":{
            "name":"SGD",
            "lr":0.025,
            "momentum":0.75
          },
          "lr_scheduler_spec":{
            "name":"LinearToZero",
            "frame":5000
          }
        }
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":5000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_self_play_no_strat_lr_002":{
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "n_steps_in_bstrap_replts":50,
          "n_bootstrapped_replications":200,
          "length_of_history":200,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":1,
          "same_init_weights":true,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":1,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"CrossEntropyLoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "normalize_inputs":true,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":2.0,
                  "end_val":0.10,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":10.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1,
                "normalize_inputs":true
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"CrossEntropyLoss",
                  "reduction":"none"
                },
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            }
          ]
        }
      },
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "n_steps_in_bstrap_replts":50,
          "n_bootstrapped_replications":200,
          "length_of_history":200,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":1,
          "same_init_weights":true,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":1,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"CrossEntropyLoss"
          },
          "contained_algorithms":[
            {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":10.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.0065,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss",
                "reduction":"none"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.0065,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_no_strat_lr_002":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":10.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss",
                "reduction":"none"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "normalize_inputs":true,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.10,
          "start_step":0,
          "end_step":500
        },
        "training_frequency":1
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[1],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "clip_grad_val":10.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.0065,
          "momentum":0.75
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":5000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_self_play_no_strat_lr_032":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":10.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss",
                "reduction":"none"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":10.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.1,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss",
                "reduction":"none"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.1,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_no_strat_lr_032":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":10.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss",
                "reduction":"none"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "normalize_inputs":true,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.10,
          "start_step":0,
          "end_step":500
        },
        "training_frequency":1
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[1],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "clip_grad_val":10.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.1,
          "momentum":0.75
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":5000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },

  "ipd_ipm_le_self_play_strat_5_lr_008":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":10.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss",
                "reduction":"none"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
    {
      "copy_n":0
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_self_play_strat_5_lr_002":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "clip_grad_val":10.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
      {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.0065,
                "momentum":0.75
              },
              "clip_grad_val":10.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.0065,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_self_play_strat_5_lr_032":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "clip_grad_val":10.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
      {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.1,
                "momentum":0.75
              },
              "clip_grad_val":10.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.1,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_strat_5_lr_008":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":10.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss",
                "reduction":"none"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "normalize_inputs":true,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.10,
          "start_step":0,
          "end_step":500
        },
        "training_frequency":1
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[1],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "clip_grad_val":10.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.025,
          "momentum":0.75
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":5000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_strat_5_lr_002":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "clip_grad_val":10.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "normalize_inputs":true,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.10,
          "start_step":0,
          "end_step":500
        },
        "training_frequency":1
      },
      "memory":{
        "name":"OnPolicyReplay"
      },
      "net":{
        "type":"MLPNet",
        "hid_layers":[1],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "clip_grad_val":10.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.0065,
          "momentum":0.75
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":5000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_strat_5_lr_032":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "clip_grad_val":10.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
              },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "normalize_inputs":true,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.10,
          "start_step":0,
          "end_step":500
        },
        "training_frequency":1
      },
      "memory":{
        "name":"OnPolicyReplay"
      },
      "net":{
        "type":"MLPNet",
        "hid_layers":[1],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "clip_grad_val":10.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.1,
          "momentum":0.75
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":5000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },

  "ipd_ipm_le_self_play_strat_2_lr_008":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":10.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss",
                "reduction":"none"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
    {
      "copy_n":0
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_self_play_strat_2_lr_002":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":10.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss",
                "reduction":"none"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
      {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.0065,
                "momentum":0.75
              },
              "clip_grad_val":10.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.0065,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_self_play_strat_2_lr_032":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":10.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss",
                "reduction":"none"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
      {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.1,
                "momentum":0.75
              },
              "clip_grad_val":10.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.1,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_strat_2_lr_008":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":10.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss",
                "reduction":"none"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "normalize_inputs":true,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.10,
          "start_step":0,
          "end_step":500
        },
        "training_frequency":1
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[1],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "clip_grad_val":10.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.025,
          "momentum":0.75
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":5000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_strat_2_lr_002":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "clip_grad_val":10.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "normalize_inputs":true,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.10,
          "start_step":0,
          "end_step":500
        },
        "training_frequency":1
      },
      "memory":{
        "name":"OnPolicyReplay"
      },
      "net":{
        "type":"MLPNet",
        "hid_layers":[1],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "clip_grad_val":10.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.0065,
          "momentum":0.75
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":5000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_strat_2_lr_032":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time": 1,
        "n_steps_in_bstrap_replts":50,
        "n_bootstrapped_replications":200,
        "length_of_history":200,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":1,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":1,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "normalize_inputs":true,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.10,
                "start_step":0,
                "end_step":500
              },
              "training_frequency":1
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "clip_grad_val":10.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
             },
              "training_frequency":1,
              "normalize_inputs":true
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[1],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.025,
                "momentum":0.75
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":5000
              }
            }
          }
        ]

      }
    },

    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "normalize_inputs":true,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.10,
          "start_step":0,
          "end_step":500
        },
        "training_frequency":1
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[1],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "clip_grad_val":10.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.1,
          "momentum":0.75
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":5000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":5000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },

  "ipd_ipm_le_vs_exploiter_no_strat_lr_008":{
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "n_steps_in_bstrap_replts":50,
          "n_bootstrapped_replications":200,
          "length_of_history":200,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":1,
          "same_init_weights":false,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":1,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"CrossEntropyLoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "normalize_inputs":true,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                 "start_val":2.0,
                  "end_val":0.10,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":10.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1,
                "normalize_inputs":true
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"CrossEntropyLoss",
                  "reduction":"none"
                },
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            }
          ]
        }
      },
      {
        "name":"LEExploiter",
        "memory":null,
        "net":null,
        "welfare_function":"default_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LEExploiter",
          "punishement_time":1,
          "n_steps_in_bstrap_replts":50,
          "n_bootstrapped_replications":200,
          "length_of_history":200,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":1,
          "same_init_weights":false,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":1,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"CrossEntropyLoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "normalize_inputs":true,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                 "start_val":2.0,
                  "end_val":0.10,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":10.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1,
                "normalize_inputs":true
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"CrossEntropyLoss",
                  "reduction":"none"
                },
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            }
          ]
        }
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":5000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_exploiter_strat_5_lr_008":{
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "n_steps_in_bstrap_replts":50,
          "n_bootstrapped_replications":200,
          "length_of_history":200,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":1,
          "same_init_weights":false,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":true,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":1,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"CrossEntropyLoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "normalize_inputs":true,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                 "start_val":2.0,
                  "end_val":0.10,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":10.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1,
                "normalize_inputs":true
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"CrossEntropyLoss",
                  "reduction":"none"
                },
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            }
          ]
        }
      },
      {
        "name":"LEExploiter",
        "memory":null,
        "net":null,
        "welfare_function":"default_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LEExploiter",
          "punishement_time":1,
          "n_steps_in_bstrap_replts":50,
          "n_bootstrapped_replications":200,
          "length_of_history":200,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":1,
          "same_init_weights":false,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":true,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":1,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"CrossEntropyLoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "normalize_inputs":true,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                 "start_val":2.0,
                  "end_val":0.10,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":10.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1,
                "normalize_inputs":true
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"CrossEntropyLoss",
                  "reduction":"none"
                },
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            }
          ]
        }
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":5000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_exploiter_strat_2_lr_008":{
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "n_steps_in_bstrap_replts":50,
          "n_bootstrapped_replications":200,
          "length_of_history":200,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":1,
          "same_init_weights":false,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":true,
          "block_len":1,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"CrossEntropyLoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "normalize_inputs":true,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                 "start_val":2.0,
                  "end_val":0.10,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":10.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1,
                "normalize_inputs":true
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"CrossEntropyLoss",
                  "reduction":"none"
                },
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            }
          ]
        }
      },
      {
        "name":"LEExploiter",
        "memory":null,
        "net":null,
        "welfare_function":"default_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LEExploiter",
          "punishement_time":1,
          "n_steps_in_bstrap_replts":50,
          "n_bootstrapped_replications":200,
          "length_of_history":200,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":1,
          "same_init_weights":false,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":true,
          "block_len":1,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"CrossEntropyLoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "normalize_inputs":true,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                 "start_val":2.0,
                  "end_val":0.10,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":10.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":500
                },
                "training_frequency":1,
                "normalize_inputs":true
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[1],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"CrossEntropyLoss",
                  "reduction":"none"
                },
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.025,
                  "momentum":0.75
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":5000
                }
              }
            }
          ]
        }
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":5000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":4,
      "log_frequency":4,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  }

}


























