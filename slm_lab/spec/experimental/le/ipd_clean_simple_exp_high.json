{
  "ipd_base_rf":{
    "world":{
      "name":"DefaultMultiAgentWorld"
    },
    "agent":[
      {
        "name":"Reinforce",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "welfare_function":"default_welfare",
        "algorithm":{
          "name":"Reinforce",
          "action_pdtype":"default",
          "action_policy":"default",
          "explore_var_spec":null,
          "gamma":0.0,
          "center_return":true,
          "normalize_return":true,
          "normalize_over_n_batch":10,
          "entropy_coef_spec":{
            "name":"linear_decay",
            "start_val":2.0,
            "end_val":0.01,
            "start_step":0,
            "end_step":7500
          },
          "training_frequency":4
        },
        "memory":{
          "name":"OnPolicyReplay"
        },
        "net":{
          "type":"MLPNet",
          "hid_layers":[4],
          "hid_layers_activation":"LeakyReLU",
          "init_fn":"normal_mu_0_std_0.1",
          "clip_grad_val":1.0,
          "optim_spec":{
            "name":"SGD",
            "lr":0.04,
            "momentum":0.9
          },
          "lr_scheduler_spec":{
            "name":"LinearToZero",
            "frame":20000
          }
        }
      },
      {
        "copy_n":0
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":20000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_trial":1
    }
  },
  "ipd_base_rf_util":{
    "world":{
      "name":"DefaultMultiAgentWorld"
    },
    "agent":[
      {
        "name":"Reinforce",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "welfare_function":"utilitarian_welfare",
        "algorithm":{
          "name":"Reinforce",
          "action_pdtype":"default",
          "action_policy":"default",
          "explore_var_spec":null,
          "gamma":0.0,
          "center_return":true,
          "normalize_return":true,
          "normalize_over_n_batch":10,
          "entropy_coef_spec":{
            "name":"linear_decay",
            "start_val":2.0,
            "end_val":0.01,
            "start_step":0,
            "end_step":7500
          },
          "training_frequency":4
        },
        "memory":{
          "name":"OnPolicyReplay"
        },
        "net":{
          "type":"MLPNet",
          "hid_layers":[4],
          "hid_layers_activation":"LeakyReLU",
          "init_fn":"normal_mu_0_std_0.1",
          "clip_grad_val":1.0,
          "optim_spec":{
            "name":"SGD",
            "lr":0.04,
            "momentum":0.9
          },
          "lr_scheduler_spec":{
            "name":"LinearToZero",
            "frame":20000
          }
        }
      },
      {
        "copy_n":0
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":20000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_trial":1
    }
  },
  "ipd_ppm_le_self_play":{
    "world":{
      "name":"DefaultMultiAgentWorld"
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "min_coop_time":0,
          "defection_detection_mode":"network_weights",
          "defection_carac_threshold":0.01,
          "average_d_carac":true,
          "average_d_carac_len":20,
          "same_init_weights":true,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":4,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"MSELoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":2.0,
                  "end_val":0.01,
                  "start_step":0,
                  "end_step":7500
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            }
          ]
        }
      },
      {
        "copy_n":0
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":20000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_trial":1
    }
  },
  "ipd_ppm_le_vs_naive_opp":{
    "world":{
      "name":"DefaultMultiAgentWorld"
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "min_coop_time":0,
          "defection_detection_mode":"network_weights",
          "defection_carac_threshold":0.01,
          "average_d_carac":true,
          "average_d_carac_len":20,
          "same_init_weights":true,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":4,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"MSELoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":2.0,
                  "end_val":0.01,
                  "start_step":0,
                  "end_step":7500
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            }
          ]
        }
      },
      {
        "name":"Reinforce",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"Reinforce",
          "action_pdtype":"default",
          "action_policy":"default",
          "explore_var_spec":null,
          "gamma":0.0,
          "center_return":true,
          "normalize_return":true,
          "normalize_over_n_batch":10,
          "entropy_coef_spec":{
            "name":"linear_decay",
            "start_val":2.0,
            "end_val":0.01,
            "start_step":0,
            "end_step":7500
          },
          "training_frequency":4
        },
        "memory":{
          "name":"OnPolicyReplay"
        },
        "net":{
          "type":"MLPNet",
          "hid_layers":[4],
          "hid_layers_activation":"LeakyReLU",
          "init_fn":"normal_mu_0_std_0.1",
          "clip_grad_val":1.0,
          "optim_spec":{
            "name":"SGD",
            "lr":0.04,
            "momentum":0.9
          },
          "lr_scheduler_spec":{
            "name":"LinearToZero",
            "frame":20000
          }
        }
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":20000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_trial":1
    }
  },

  "ipd_ipm_le_self_play_no_strat_lr_008":{
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":20,
          "same_init_weights":true,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":4,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"MSELoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":2.0,
                  "end_val":0.01,
                  "start_step":0,
                  "end_step":7500
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"MSELoss",
                  "reduction":"none"
                },
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":null
              }
            }
          ]
        }
      },
      {
        "copy_n":0
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":20000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_no_strat_lr_008":{
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":20,
          "same_init_weights":true,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":4,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"MSELoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":2.0,
                  "end_val":0.01,
                  "start_step":0,
                  "end_step":7500
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"MSELoss",
                  "reduction":"none"
                },
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":null
              }
            }
          ]
        }
      },
      {
        "name":"Reinforce",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"Reinforce",
          "action_pdtype":"default",
          "action_policy":"default",
          "explore_var_spec":null,
          "gamma":0.0,
          "center_return":true,
          "normalize_return":true,
          "normalize_over_n_batch":10,
          "entropy_coef_spec":{
            "name":"linear_decay",
            "start_val":2.0,
            "end_val":0.01,
            "start_step":0,
            "end_step":7500
          },
          "training_frequency":4
        },
        "memory":{
          "name":"OnPolicyReplay"
        },
        "net":{
          "type":"MLPNet",
          "hid_layers":[4],
          "hid_layers_activation":"LeakyReLU",
          "init_fn":"normal_mu_0_std_0.1",
          "clip_grad_val":1.0,
          "optim_spec":{
            "name":"SGD",
            "lr":0.04,
            "momentum":0.9
          },
          "lr_scheduler_spec":{
            "name":"LinearToZero",
            "frame":20000
          }
        }
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":20000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_self_play_no_strat_lr_002":{
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":20,
          "same_init_weights":true,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":4,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"MSELoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":2.0,
                  "end_val":0.01,
                  "start_step":0,
                  "end_step":7500
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"MSELoss",
                  "reduction":"none"
                },
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            }
          ]
        }
      },
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":20,
          "same_init_weights":true,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "block_len":4,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"MSELoss"
          },
          "contained_algorithms":[
            {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.01,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":5000
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"MSELoss",
                "reduction":"none"
              },
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.01,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_no_strat_lr_002":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"MSELoss",
                "reduction":"none"
              },
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.01,
          "start_step":0,
          "end_step":7500
        },
        "training_frequency":4
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[4],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "clip_grad_val":1.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.01,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":20000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_self_play_no_strat_lr_032":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"MSELoss",
                "reduction":"none"
              },
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.12,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"MSELoss",
                "reduction":"none"
              },
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.12,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_no_strat_lr_032":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"MSELoss",
                "reduction":"none"
              },
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.01,
          "start_step":0,
          "end_step":7500
        },
        "training_frequency":4
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[4],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "clip_grad_val":1.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.12,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":20000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },

  "ipd_ipm_le_self_play_strat_5_lr_008":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"MSELoss",
                "reduction":"none"
              },
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
    {
      "copy_n":0
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_self_play_strat_5_lr_002":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "clip_grad_val":1.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
              "loss_spec":{
                "name":"MSELoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
      {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.01,
                "momentum":0.9
              },
              "clip_grad_val":1.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
              "loss_spec":{
                "name":"MSELoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.01,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_self_play_strat_5_lr_032":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "clip_grad_val":1.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
              "loss_spec":{
                "name":"MSELoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
      {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.12,
                "momentum":0.9
              },
              "clip_grad_val":1.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "loss_spec":{
                "name":"MSELoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.12,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_strat_5_lr_008":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"MSELoss",
                "reduction":"none"
              },
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.01,
          "start_step":0,
          "end_step":7500
        },
        "training_frequency":4
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[4],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "clip_grad_val":1.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.04,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":20000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_strat_5_lr_002":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "clip_grad_val":1.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
              "loss_spec":{
                "name":"MSELoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.01,
          "start_step":0,
          "end_step":7500
        },
        "training_frequency":4
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[4],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "loss_spec":{
          "name":"MSELoss"
        },
        "clip_grad_val":1.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.01,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":20000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_strat_5_lr_032":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "clip_grad_val":1.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "loss_spec":{
                "name":"MSELoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.01,
          "start_step":0,
          "end_step":7500
        },
        "training_frequency":4
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[4],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "loss_spec":{
          "name":"MSELoss"
        },
        "clip_grad_val":1.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.12,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":20000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },

  "ipd_ipm_le_self_play_strat_2_lr_008":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"MSELoss",
                "reduction":"none"
              },
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
    {
      "copy_n":0
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_self_play_strat_2_lr_002":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"MSELoss",
                "reduction":"none"
              },
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
      {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.01,
                "momentum":0.9
              },
              "clip_grad_val":1.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
              "loss_spec":{
                "name":"MSELoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.01,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_self_play_strat_2_lr_032":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"MSELoss",
                "reduction":"none"
              },
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
      {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.12,
                "momentum":0.9
              },
              "clip_grad_val":1.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
              "loss_spec":{
                "name":"MSELoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.12,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_strat_2_lr_008":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "loss_spec":{
                "name":"MSELoss",
                "reduction":"none"
              },
              "clip_grad_val":1.0,
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.01,
          "start_step":0,
          "end_step":7500
        },
        "training_frequency":4
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[4],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "clip_grad_val":1.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.04,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":20000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_strat_2_lr_002":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "clip_grad_val":1.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
              "loss_spec":{
                "name":"MSELoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },
    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.01,
          "start_step":0,
          "end_step":7500
        },
        "training_frequency":4
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[4],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "loss_spec":{
          "name":"MSELoss"
        },
        "clip_grad_val":1.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.01,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":20000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_naive_opp_strat_2_lr_032":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":false},
    "agent":[
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "same_init_weights":true,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":4,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"MSELoss"
        },
        "contained_algorithms":[
          {
            "name":"Reinforce",
            "algorithm":{
              "name":"Reinforce",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.0,
              "center_return":true,
              "normalize_return":true,
              "normalize_over_n_batch":10,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":2.0,
                "end_val":0.01,
                "start_step":0,
                "end_step":7500
              },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "clip_grad_val":1.0,
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }

          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
             "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
             },
              "training_frequency":4
            },
            "memory":{
              "name":"OnPolicyReplay"
            },
            "net":{
              "type":"MLPNet",
              "hid_layers":[4],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"normal_mu_0_std_0.1",
              "clip_grad_val":1.0,
              "loss_spec":{
                "name":"MSELoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.04,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":20000
              }
            }
          }
        ]

      }
    },

    {
      "name":"Reinforce",
      "observing_other_agents":{"name":"FullyObservable"},
      "algorithm":{
        "name":"Reinforce",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.0,
        "center_return":true,
        "normalize_return":true,
        "normalize_over_n_batch":10,
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":2.0,
          "end_val":0.01,
          "start_step":0,
          "end_step":7500
        },
        "training_frequency":4
      },


      "memory":{
        "name":"OnPolicyReplay"
      },


      "net":{
        "type":"MLPNet",
        "hid_layers":[4],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"normal_mu_0_std_0.1",
        "loss_spec":{
          "name":"MSELoss"
        },
        "clip_grad_val":1.0,
        "optim_spec":{
          "name":"SGD",
          "lr":0.12,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":20000
        }
      }
    }
    ],
    "env":[{
      "name":"IteratedPrisonersDilemma-v0",
      "max_t":null,
      "max_frame":20000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },

  "ipd_ipm_le_vs_exploiter_no_strat_lr_008":{
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":20,
          "same_init_weights":false,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "length_of_history":200,
          "block_len":4,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"MSELoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                 "start_val":2.0,
                  "end_val":0.01,
                  "start_step":0,
                  "end_step":7500
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"MSELoss",
                  "reduction":"none"
                },
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":null
              }
            }
          ]
        }
      },
      {
        "name":"LEExploiter",
        "memory":null,
        "net":null,
        "welfare_function":"default_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LEExploiter",
          "punishement_time":1,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":20,
          "same_init_weights":false,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "length_of_history":200,
          "block_len":4,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"MSELoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                 "start_val":2.0,
                  "end_val":0.01,
                  "start_step":0,
                  "end_step":7500
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"MSELoss",
                  "reduction":"none"
                },
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":null
              }
            }
          ]
        }
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":20000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_exploiter_strat_5_lr_008":{
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":20,
          "same_init_weights":false,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":true,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "length_of_history":200,
          "block_len":4,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"MSELoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                 "start_val":2.0,
                  "end_val":0.01,
                  "start_step":0,
                  "end_step":7500
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"MSELoss",
                  "reduction":"none"
                },
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            }
          ]
        }
      },
      {
        "name":"LEExploiter",
        "memory":null,
        "net":null,
        "welfare_function":"default_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LEExploiter",
          "punishement_time":1,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":20,
          "same_init_weights":false,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":true,
          "strat_5_coeff":10.0,
          "use_strat_2":false,
          "length_of_history":200,
          "block_len":4,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"MSELoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                 "start_val":2.0,
                  "end_val":0.01,
                  "start_step":0,
                  "end_step":7500
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"MSELoss",
                  "reduction":"none"
                },
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            }
          ]
        }
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":20000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "ipd_ipm_le_vs_exploiter_strat_2_lr_008":{
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LE",
          "punishement_time":1,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":20,
          "same_init_weights":false,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":true,
          "length_of_history":200,
          "block_len":4,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"MSELoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                 "start_val":2.0,
                  "end_val":0.01,
                  "start_step":0,
                  "end_step":7500
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"MSELoss",
                  "reduction":"none"
                },
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            }
          ]
        }
      },
      {
        "name":"LEExploiter",
        "memory":null,
        "net":null,
        "welfare_function":"default_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"LEExploiter",
          "punishement_time":1,
          "min_coop_time":0,
          "defection_detection_mode":"spl_observed_actions",
          "defection_carac_threshold":0.0,
          "average_d_carac":true,
          "average_d_carac_len":20,
          "same_init_weights":false,
          "coop_net_ent_diff_as_lr":false,
          "use_sl_for_simu_coop":false,
          "use_strat_4":false,
          "use_strat_5":false,
          "strat_5_coeff":10.0,
          "use_strat_2":true,
          "length_of_history":200,
          "block_len":4,
          "meta_algo_memory":{
            "name":"Replay",
            "batch_size":1,
            "max_size":-1
          },
          "meta_algo_loss":{
            "name":"MSELoss"
          },
          "contained_algorithms":[
            {
              "name":"Reinforce",
              "algorithm":{
                "name":"Reinforce",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "gamma":0.0,
                "center_return":true,
                "normalize_return":true,
                "normalize_over_n_batch":10,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                 "start_val":2.0,
                  "end_val":0.01,
                  "start_step":0,
                  "end_step":7500
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            },
            {
              "copy_n":0
            },
            {
              "copy_n":0
            },
            {
              "name":"SupervisedLAPolicy",
              "algorithm":{
                "name":"SupervisedLAPolicy",
                "action_pdtype":"default",
                "action_policy":"default",
                "explore_var_spec":null,
                "entropy_coef_spec":{
                  "name":"linear_decay",
                  "start_val":0.0,
                  "end_val":0.0,
                  "start_step":0,
                  "end_step":5000
                },
                "training_frequency":4
              },
              "memory":{
                "name":"OnPolicyReplay"
              },
              "net":{
                "type":"MLPNet",
                "hid_layers":[4],
                "hid_layers_activation":"LeakyReLU",
                "init_fn":"normal_mu_0_std_0.1",
                "loss_spec":{
                  "name":"MSELoss",
                  "reduction":"none"
                },
                "clip_grad_val":1.0,
                "optim_spec":{
                  "name":"SGD",
                  "lr":0.04,
                  "momentum":0.9
                },
                "lr_scheduler_spec":{
                  "name":"LinearToZero",
                  "frame":20000
                }
              }
            }
          ]
        }
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":20000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },

  "ipd_mb_mpc_cem_le_self_play":{
    "world":{
      "name":"DefaultMultiAgentWorld"
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm": {
          "name": "LE",
          "punishement_time": 1,
          "min_coop_time": 0,
          "defection_detection_mode": "spl_observed_actions",
          "defection_carac_threshold": 0.0,
          "average_d_carac": true,
          "average_d_carac_len": 20,
          "same_init_weights": true,
          "coop_net_ent_diff_as_lr": false,
          "use_sl_for_simu_coop": false,
          "use_strat_4": false,
          "use_strat_5": false,
          "strat_5_coeff": 10.0,
          "use_strat_2": false,
          "block_len": 4,
          "meta_algo_memory": {
            "name": "Replay",
            "batch_size": 1,
            "max_size": -1
          },
          "meta_algo_loss": {
            "name": "MSELoss"
          },
          "contained_algorithms": [
            {
              "name": "PETS",
              "algorithm": {
                "name": "PETS",
                "contained_algorithms": [
                  {
                    "name": "SupervisedLAPolicy",
                    "targets":"rewards",
                    "inputs":["next_states", "actions"],
                    "algorithm": {
                      "name": "SupervisedLAPolicy",
                      "action_pdtype": "default",
                      "action_policy": "default",
                      "explore_var_spec": null,
                      "entropy_coef_spec": {
                        "name": "linear_decay",
                        "start_val": 0.0,
                        "end_val": 0.0,
                        "start_step": 0,
                        "end_step": 5000
                      },
                      "training_frequency": 4
                    },
                    "memory": {
                      "name": "OnPolicyReplay"
                    },
                    "net": {
                      "type": "MLPNet",
                      "hid_layers": [
                        4
                      ],
                      "hid_layers_activation": "LeakyReLU",
                      "init_fn": "normal_mu_0_std_0.1",
                      "loss_spec": {
                        "name": "MSELoss",
                        "reduction": "none"
                      },
                      "clip_grad_val": 1.0,
                      "optim_spec": {
                        "name": "SGD",
                        "lr": 0.008,
                        "momentum": 0.9
                      },
                      "lr_scheduler_spec": {
                        "name": "LinearToZero",
                        "frame": 20000
                      }
                    }
                  },
                  {
                    "name": "SPLearningBayesianPolicy",
                    "targets":"states",
                    "algorithm": {
                      "name": "SPLearningBayesianPolicy",
                      "action_pdtype": "default",
                      "action_policy": "default",
                      "explore_var_spec": null,
                      "entropy_coef_spec": {
                        "name": "linear_decay",
                        "start_val": 0.0,
                        "end_val": 0.0,
                        "start_step": 0,
                        "end_step": 5000
                      },
                      "training_frequency": 4
                    },
                    "memory": {
                      "name": "OnPolicyReplay"
                    },
                    "net": {
                      "type": "MLPNet",
                      "hid_layers": [
                        4
                      ],
                      "hid_layers_activation": "LeakyReLU",
                      "init_fn": "normal_mu_0_std_0.1",
                      "loss_spec": {
                        "name": "MSELoss",
                        "reduction": "none"
                      },
                      "clip_grad_val": 1.0,
                      "optim_spec": {
                        "name": "SGD",
                        "lr": 0.008,
                        "momentum": 0.9
                      },
                      "lr_scheduler_spec": {
                        "name": "LinearToZero",
                        "frame": 20000
                      }
                    }
                  },
                  {
                    "copy_n": 0
                  }
                ]
              }
            },
            {
              "copy_n": 0
            },
            {
              "copy_n": 0
            },
            {
              "name": "SupervisedLAPolicy",
              "algorithm": {
                "name": "SupervisedLAPolicy",
                "action_pdtype": "default",
                "action_policy": "default",
                "explore_var_spec": null,
                "entropy_coef_spec": {
                  "name": "linear_decay",
                  "start_val": 0.0,
                  "end_val": 0.0,
                  "start_step": 0,
                  "end_step": 5000
                },
                "training_frequency": 4
              },
              "memory": {
                "name": "OnPolicyReplay"
              },
              "net": {
                "type": "MLPNet",
                "hid_layers": [
                  4
                ],
                "hid_layers_activation": "LeakyReLU",
                "init_fn": "normal_mu_0_std_0.1",
                "loss_spec": {
                  "name": "MSELoss",
                  "reduction": "none"
                },
                "clip_grad_val": 1.0,
                "optim_spec": {
                  "name": "SGD",
                  "lr": 0.008,
                  "momentum": 0.9
                },
                "lr_scheduler_spec": {
                  "name": "LinearToZero",
                  "frame": 20000
                }
              }
            }
          ]
        }
      },
      {
        "copy_n":0
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":20000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_trial":1
    }
  },
  "ipd_mb_mpc_cem_le_vs_naive_opp":{
    "world":{
      "name":"DefaultMultiAgentWorld"
    },
    "agent":[
      {
        "name":"LE",
        "memory":null,
        "net":null,
        "welfare_function":"utilitarian_welfare",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm": {
          "name": "LE",
          "punishement_time": 1,
          "min_coop_time": 0,
          "defection_detection_mode": "spl_observed_actions",
          "defection_carac_threshold": 0.0,
          "average_d_carac": true,
          "average_d_carac_len": 20,
          "same_init_weights": true,
          "coop_net_ent_diff_as_lr": false,
          "use_sl_for_simu_coop": false,
          "use_strat_4": false,
          "use_strat_5": false,
          "strat_5_coeff": 10.0,
          "use_strat_2": false,
          "block_len": 4,
          "meta_algo_memory": {
            "name": "Replay",
            "batch_size": 1,
            "max_size": -1
          },
          "meta_algo_loss": {
            "name": "MSELoss"
          },
          "contained_algorithms": [
            {
              "name": "PETS",
              "contained_algorithms": [
                {
                  "name": "SupervisedLAPolicy",
                  "targets":"rewards",
                  "inputs":["next_states", "actions"],
                  "algorithm": {
                    "name": "SupervisedLAPolicy",
                    "action_pdtype": "default",
                    "action_policy": "default",
                    "explore_var_spec": null,
                    "entropy_coef_spec": {
                      "name": "linear_decay",
                      "start_val": 0.0,
                      "end_val": 0.0,
                      "start_step": 0,
                      "end_step": 5000
                    },
                    "training_frequency": 4
                  },
                  "memory": {
                    "name": "OnPolicyReplay"
                  },
                  "net": {
                    "type": "MLPNet",
                    "hid_layers": [
                      4
                    ],
                    "hid_layers_activation": "LeakyReLU",
                    "init_fn": "normal_mu_0_std_0.1",
                    "loss_spec": {
                      "name": "MSELoss",
                      "reduction": "none"
                    },
                    "clip_grad_val": 1.0,
                    "optim_spec": {
                      "name": "SGD",
                      "lr": 0.008,
                      "momentum": 0.9
                    },
                    "lr_scheduler_spec": {
                      "name": "LinearToZero",
                      "frame": 20000
                    }
                  }
                },
                {
                  "name": "SPLearningBayesianPolicy",
                  "targets":"states",
                  "algorithm": {
                    "name": "SPLearningBayesianPolicy",
                    "action_pdtype": "default",
                    "action_policy": "default",
                    "explore_var_spec": null,
                    "entropy_coef_spec": {
                      "name": "linear_decay",
                      "start_val": 0.0,
                      "end_val": 0.0,
                      "start_step": 0,
                      "end_step": 5000
                    },
                    "training_frequency": 4
                  },
                  "memory": {
                    "name": "OnPolicyReplay"
                  },
                  "net": {
                    "type": "MLPNet",
                    "hid_layers": [
                      4
                    ],
                    "hid_layers_activation": "LeakyReLU",
                    "init_fn": "normal_mu_0_std_0.1",
                    "loss_spec": {
                      "name": "MSELoss",
                      "reduction": "none"
                    },
                    "clip_grad_val": 1.0,
                    "optim_spec": {
                      "name": "SGD",
                      "lr": 0.008,
                      "momentum": 0.9
                    },
                    "lr_scheduler_spec": {
                      "name": "LinearToZero",
                      "frame": 20000
                    }
                  }
                },
                {
                  "copy_n": 0
                }
              ]
            },
            {
              "copy_n": 0
            },
            {
              "copy_n": 0
            },
            {
              "name": "SupervisedLAPolicy",
              "algorithm": {
                "name": "SupervisedLAPolicy",
                "action_pdtype": "default",
                "action_policy": "default",
                "explore_var_spec": null,
                "entropy_coef_spec": {
                  "name": "linear_decay",
                  "start_val": 0.0,
                  "end_val": 0.0,
                  "start_step": 0,
                  "end_step": 5000
                },
                "training_frequency": 4
              },
              "memory": {
                "name": "OnPolicyReplay"
              },
              "net": {
                "type": "MLPNet",
                "hid_layers": [
                  4
                ],
                "hid_layers_activation": "LeakyReLU",
                "init_fn": "normal_mu_0_std_0.1",
                "loss_spec": {
                  "name": "MSELoss",
                  "reduction": "none"
                },
                "clip_grad_val": 1.0,
                "optim_spec": {
                  "name": "SGD",
                  "lr": 0.008,
                  "momentum": 0.9
                },
                "lr_scheduler_spec": {
                  "name": "LinearToZero",
                  "frame": 20000
                }
              }
            }
          ]
        }
      },
      {
        "name":"Reinforce",
        "observing_other_agents":{
          "name":"FullyObservable"
        },
        "algorithm":{
          "name":"Reinforce",
          "action_pdtype":"default",
          "action_policy":"default",
          "explore_var_spec":null,
          "gamma":0.0,
          "center_return":true,
          "normalize_return":true,
          "normalize_over_n_batch":10,
          "entropy_coef_spec":{
            "name":"linear_decay",
            "start_val":2.0,
            "end_val":0.01,
            "start_step":0,
            "end_step":7500
          },
          "training_frequency":4
        },
        "memory":{
          "name":"OnPolicyReplay"
        },
        "net":{
          "type":"MLPNet",
          "hid_layers":[4],
          "hid_layers_activation":"LeakyReLU",
          "init_fn":"normal_mu_0_std_0.1",
          "clip_grad_val":1.0,
          "optim_spec":{
            "name":"SGD",
            "lr":0.04,
            "momentum":0.9
          },
          "lr_scheduler_spec":{
            "name":"LinearToZero",
            "frame":20000
          }
        }
      }
    ],
    "env":[
      {
        "name":"IteratedPrisonersDilemma-v0",
        "max_t":null,
        "max_frame":20000,
        "num_envs":1
      }
    ],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":20,
      "log_frequency":20,
      "max_session":1,
      "max_trial":1
    }
  }

}


























