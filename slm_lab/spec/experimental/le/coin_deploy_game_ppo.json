{

  "coin_deploy_game_ppo_pm_default_or_util": {
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent": [{
      "name": "DeploymentGame",
      "memory": null,
      "net": null,
      "observing_other_agents": {
        "name": "FullyObservable"
      },
      "algorithm": {
        "name": "DeploymentGame",
        "redeploy_every_n_epi": 4000,
        "deploy_algo":{
          "name":"UCB1",
          "k_choice":2
        },
        "contained_algorithms": [
            {
              "name": "PPO",
              "welfare_function":"default_welfare",
              "observing_other_agents":{"name":"FullyObservable"},
              "algorithm": {
                "name": "PPO",
                "action_pdtype": "default",
                "action_policy": "default",
                "explore_var_spec": null,
                "gamma": 0.5,
                "lam": 0.95,
                "clip_eps_spec": {
                  "name": "linear_decay",
                  "start_val": 0.05,
                  "end_val": 0.005,
                  "start_step": 10000,
                  "end_step": 80000
                },
                "entropy_coef_spec": {
                  "name": "linear_decay",
                  "start_val": 0.3,
                  "end_val": 0.06,
                  "start_step": 0,
                  "end_step": 100000
                },
                "val_loss_coef": 1.0,
                "time_horizon": 128,
                "minibatch_size": 32,
                "training_epoch": 8
              },
              "memory": {
                "name": "OnPolicyBatchReplay"
              },
              "net": {
                "type": "ConvNet",
                "conv_hid_layers": [
                  [16, 3, 1, 1, 1],
                  [32, 3, 1, 0, 1]
                ],
                "shared": true,
                "fc_hid_layers": [16],
                "hid_layers_activation": "relu",
                "init_fn": "orthogonal_",
                "normalize": false,
                "batch_norm": false,
                "clip_grad_val": 0.5,
                "use_same_optim": false,
                "actor_optim_spec": {
                  "name": "Adam",
                  "lr": 8e-4
                },
                "critic_optim_spec": {
                  "name": "Adam",
                  "lr": 8e-4
                },
                "lr_scheduler_spec": {
                  "name": "LinearToZero",
                  "frame": 100000
                },
                "gpu": false
              }
            },
            {
              "name": "PPO",
              "welfare_function":"utilitarian_welfare",
              "observing_other_agents":{"name":"FullyObservable"},
              "algorithm": {
                "name": "PPO",
                "action_pdtype": "default",
                "action_policy": "default",
                "explore_var_spec": null,
                "gamma": 0.5,
                "lam": 0.95,
                "clip_eps_spec": {
                  "name": "linear_decay",
                  "start_val": 0.05,
                  "end_val": 0.005,
                  "start_step": 10000,
                  "end_step": 80000
                },
                "entropy_coef_spec": {
                  "name": "linear_decay",
                  "start_val": 0.3,
                  "end_val": 0.06,
                  "start_step": 0,
                  "end_step": 100000
                },
                "val_loss_coef": 1.0,
                "time_horizon": 128,
                "minibatch_size": 32,
                "training_epoch": 8
              },
              "memory": {
                "name": "OnPolicyBatchReplay"
              },
              "net": {
                "type": "ConvNet",
                "conv_hid_layers": [
                  [16, 3, 1, 1, 1],
                  [32, 3, 1, 0, 1]
                ],
                "shared": true,
                "fc_hid_layers": [16],
                "hid_layers_activation": "relu",
                "init_fn": "orthogonal_",
                "normalize": false,
                "batch_norm": false,
                "clip_grad_val": 0.5,
                "use_same_optim": false,
                "actor_optim_spec": {
                  "name": "Adam",
                  "lr": 8e-4
                },
                "critic_optim_spec": {
                  "name": "Adam",
                  "lr": 8e-4
                },
                "lr_scheduler_spec": {
                  "name": "LinearToZero",
                  "frame": 100000
                },
                "gpu": false
              }
            }
          ]
        }
      },
      {
        "copy_n": 0
      }
    ],
    "env": [{
      "name": "CoinGame-v0",
      "max_t": null,
      "max_frame": 8000000,
      "num_envs": 1
    }],
    "body": {
      "product": "outer",
      "num": 1
    },
    "meta": {
      "distributed": false,
      "eval_frequency": 10000,
      "log_frequency": 10000,
      "max_session": 10,
      "max_concurrent_session":10,
      "max_trial": 1
    }
  },









  "coin_deploy_game_ppo_pm_default_vs_default_or_util": {
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent": [{
      "name": "DeploymentGame",
      "memory": null,
      "net": null,
      "observing_other_agents": {
        "name": "FullyObservable"
      },
      "algorithm": {
        "name": "DeploymentGame",
        "redeploy_every_n_epi": 4000,
        "deploy_algo":{
          "name":"UCB1",
          "k_choice":1
        },
        "contained_algorithms": [
          {
            "name": "PPO",
            "welfare_function":"default_welfare",
            "observing_other_agents":{"name":"FullyObservable"},
            "algorithm": {
              "name": "PPO",
              "action_pdtype": "default",
              "action_policy": "default",
              "explore_var_spec": null,
              "gamma": 0.5,
              "lam": 0.95,
              "clip_eps_spec": {
                "name": "linear_decay",
                "start_val": 0.05,
                "end_val": 0.005,
                "start_step": 10000,
                "end_step": 80000
              },
              "entropy_coef_spec": {
                "name": "linear_decay",
                "start_val": 0.3,
                "end_val": 0.06,
                "start_step": 0,
                "end_step": 100000
              },
              "val_loss_coef": 1.0,
              "time_horizon": 128,
              "minibatch_size": 32,
              "training_epoch": 8
            },
            "memory": {
              "name": "OnPolicyBatchReplay"
            },
            "net": {
              "type": "ConvNet",
              "conv_hid_layers": [
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared": true,
              "fc_hid_layers": [16],
              "hid_layers_activation": "relu",
              "init_fn": "orthogonal_",
              "normalize": false,
              "batch_norm": false,
              "clip_grad_val": 0.5,
              "use_same_optim": false,
              "actor_optim_spec": {
                "name": "Adam",
                "lr": 8e-4
              },
              "critic_optim_spec": {
                "name": "Adam",
                "lr": 8e-4
              },
              "lr_scheduler_spec": {
                "name": "LinearToZero",
                "frame": 100000
              },
              "gpu": false
            }
          }
          ]
        }
      },
      {
        "name": "DeploymentGame",
        "memory": null,
        "net": null,
        "observing_other_agents": {
          "name": "FullyObservable"
        },
        "algorithm": {
          "name": "DeploymentGame",
          "redeploy_every_n_epi": 4000,
          "deploy_algo":{
            "name":"UCB1",
            "k_choice":2
          },
          "contained_algorithms": [
            {
              "name": "PPO",
              "welfare_function":"default_welfare",
              "observing_other_agents":{"name":"FullyObservable"},
              "algorithm": {
                "name": "PPO",
                "action_pdtype": "default",
                "action_policy": "default",
                "explore_var_spec": null,
                "gamma": 0.5,
                "lam": 0.95,
                "clip_eps_spec": {
                  "name": "linear_decay",
                  "start_val": 0.05,
                  "end_val": 0.005,
                  "start_step": 10000,
                  "end_step": 80000
                },
                "entropy_coef_spec": {
                  "name": "linear_decay",
                  "start_val": 0.3,
                  "end_val": 0.06,
                  "start_step": 0,
                  "end_step": 100000
                },
                "val_loss_coef": 1.0,
                "time_horizon": 128,
                "minibatch_size": 32,
                "training_epoch": 8
              },
              "memory": {
                "name": "OnPolicyBatchReplay"
              },
              "net": {
                "type": "ConvNet",
                "conv_hid_layers": [
                  [16, 3, 1, 1, 1],
                  [32, 3, 1, 0, 1]
                ],
                "shared": true,
                "fc_hid_layers": [16],
                "hid_layers_activation": "relu",
                "init_fn": "orthogonal_",
                "normalize": false,
                "batch_norm": false,
                "clip_grad_val": 0.5,
                "use_same_optim": false,
                "actor_optim_spec": {
                  "name": "Adam",
                  "lr": 8e-4
                },
                "critic_optim_spec": {
                  "name": "Adam",
                  "lr": 8e-4
                },
                "lr_scheduler_spec": {
                  "name": "LinearToZero",
                  "frame": 100000
                },
                "gpu": false
              }
            },
            {
              "name": "PPO",
              "welfare_function":"utilitarian_welfare",
              "observing_other_agents":{"name":"FullyObservable"},
              "algorithm": {
                "name": "PPO",
                "action_pdtype": "default",
                "action_policy": "default",
                "explore_var_spec": null,
                "gamma": 0.5,
                "lam": 0.95,
                "clip_eps_spec": {
                  "name": "linear_decay",
                  "start_val": 0.05,
                  "end_val": 0.005,
                  "start_step": 10000,
                  "end_step": 80000
                },
                "entropy_coef_spec": {
                  "name": "linear_decay",
                  "start_val": 0.3,
                  "end_val": 0.06,
                  "start_step": 0,
                  "end_step": 100000
                },
                "val_loss_coef": 1.0,
                "time_horizon": 128,
                "minibatch_size": 32,
                "training_epoch": 8
              },
              "memory": {
                "name": "OnPolicyBatchReplay"
              },
              "net": {
                "type": "ConvNet",
                "conv_hid_layers": [
                  [16, 3, 1, 1, 1],
                  [32, 3, 1, 0, 1]
                ],
                "shared": true,
                "fc_hid_layers": [16],
                "hid_layers_activation": "relu",
                "init_fn": "orthogonal_",
                "normalize": false,
                "batch_norm": false,
                "clip_grad_val": 0.5,
                "use_same_optim": false,
                "actor_optim_spec": {
                  "name": "Adam",
                  "lr": 8e-4
                },
                "critic_optim_spec": {
                  "name": "Adam",
                  "lr": 8e-4
                },
                "lr_scheduler_spec": {
                  "name": "LinearToZero",
                  "frame": 100000
                },
                "gpu": false
              }
            }
          ]
        }
      }
    ],
    "env": [{
      "name": "CoinGame-v0",
      "max_t": null,
      "max_frame": 8000000,
      "num_envs": 1
    }],
    "body": {
      "product": "outer",
      "num": 1
    },
    "meta": {
      "distributed": false,
      "eval_frequency": 10000,
      "log_frequency": 10000,
      "max_session": 10,
      "max_concurrent_session":10,
      "max_trial": 1
    }
  },














  "coin_deploy_game_ppo_pm_util_vs_default_or_util": {
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent": [{
      "name": "DeploymentGame",
      "memory": null,
      "net": null,
      "observing_other_agents": {
        "name": "FullyObservable"
      },
      "algorithm": {
        "name": "DeploymentGame",
        "redeploy_every_n_epi": 4000,
        "deploy_algo":{
          "name":"UCB1",
          "k_choice":1
        },
        "contained_algorithms": [
          {
            "name": "PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{"name":"FullyObservable"},
            "algorithm": {
              "name": "PPO",
              "action_pdtype": "default",
              "action_policy": "default",
              "explore_var_spec": null,
              "gamma": 0.5,
              "lam": 0.95,
              "clip_eps_spec": {
                "name": "linear_decay",
                "start_val": 0.05,
                "end_val": 0.005,
                "start_step": 10000,
                "end_step": 80000
              },
              "entropy_coef_spec": {
                "name": "linear_decay",
                "start_val": 0.3,
                "end_val": 0.06,
                "start_step": 0,
                "end_step": 100000
              },
              "val_loss_coef": 1.0,
              "time_horizon": 128,
              "minibatch_size": 32,
              "training_epoch": 8
            },
            "memory": {
              "name": "OnPolicyBatchReplay"
            },
            "net": {
              "type": "ConvNet",
              "conv_hid_layers": [
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared": true,
              "fc_hid_layers": [16],
              "hid_layers_activation": "relu",
              "init_fn": "orthogonal_",
              "normalize": false,
              "batch_norm": false,
              "clip_grad_val": 0.5,
              "use_same_optim": false,
              "actor_optim_spec": {
                "name": "Adam",
                "lr": 8e-4
              },
              "critic_optim_spec": {
                "name": "Adam",
                "lr": 8e-4
              },
              "lr_scheduler_spec": {
                "name": "LinearToZero",
                "frame": 100000
              },
              "gpu": false
            }
          }
          ]
        }
      },
      {
        "name": "DeploymentGame",
        "memory": null,
        "net": null,
        "observing_other_agents": {
          "name": "FullyObservable"
        },
        "algorithm": {
          "name": "DeploymentGame",
          "redeploy_every_n_epi": 4000,
          "deploy_algo":{
            "name":"UCB1",
            "k_choice":2
          },
          "contained_algorithms": [
            {
              "name": "PPO",
              "welfare_function":"default_welfare",
              "observing_other_agents":{"name":"FullyObservable"},
              "algorithm": {
                "name": "PPO",
                "action_pdtype": "default",
                "action_policy": "default",
                "explore_var_spec": null,
                "gamma": 0.5,
                "lam": 0.95,
                "clip_eps_spec": {
                  "name": "linear_decay",
                  "start_val": 0.05,
                  "end_val": 0.005,
                  "start_step": 10000,
                  "end_step": 80000
                },
                "entropy_coef_spec": {
                  "name": "linear_decay",
                  "start_val": 0.3,
                  "end_val": 0.06,
                  "start_step": 0,
                  "end_step": 100000
                },
                "val_loss_coef": 1.0,
                "time_horizon": 128,
                "minibatch_size": 32,
                "training_epoch": 8
              },
              "memory": {
                "name": "OnPolicyBatchReplay"
              },
              "net": {
                "type": "ConvNet",
                "conv_hid_layers": [
                  [16, 3, 1, 1, 1],
                  [32, 3, 1, 0, 1]
                ],
                "shared": true,
                "fc_hid_layers": [16],
                "hid_layers_activation": "relu",
                "init_fn": "orthogonal_",
                "normalize": false,
                "batch_norm": false,
                "clip_grad_val": 0.5,
                "use_same_optim": false,
                "actor_optim_spec": {
                  "name": "Adam",
                  "lr": 8e-4
                },
                "critic_optim_spec": {
                  "name": "Adam",
                  "lr": 8e-4
                },
                "lr_scheduler_spec": {
                  "name": "LinearToZero",
                  "frame": 100000
                },
                "gpu": false
              }
            },
            {
              "name": "PPO",
              "welfare_function":"utilitarian_welfare",
              "observing_other_agents":{"name":"FullyObservable"},
              "algorithm": {
                "name": "PPO",
                "action_pdtype": "default",
                "action_policy": "default",
                "explore_var_spec": null,
                "gamma": 0.5,
                "lam": 0.95,
                "clip_eps_spec": {
                  "name": "linear_decay",
                  "start_val": 0.05,
                  "end_val": 0.005,
                  "start_step": 10000,
                  "end_step": 80000
                },
                "entropy_coef_spec": {
                  "name": "linear_decay",
                  "start_val": 0.3,
                  "end_val": 0.06,
                  "start_step": 0,
                  "end_step": 100000
                },
                "val_loss_coef": 1.0,
                "time_horizon": 128,
                "minibatch_size": 32,
                "training_epoch": 8
              },
              "memory": {
                "name": "OnPolicyBatchReplay"
              },
              "net": {
                "type": "ConvNet",
                "conv_hid_layers": [
                  [16, 3, 1, 1, 1],
                  [32, 3, 1, 0, 1]
                ],
                "shared": true,
                "fc_hid_layers": [16],
                "hid_layers_activation": "relu",
                "init_fn": "orthogonal_",
                "normalize": false,
                "batch_norm": false,
                "clip_grad_val": 0.5,
                "use_same_optim": false,
                "actor_optim_spec": {
                  "name": "Adam",
                  "lr": 8e-4
                },
                "critic_optim_spec": {
                  "name": "Adam",
                  "lr": 8e-4
                },
                "lr_scheduler_spec": {
                  "name": "LinearToZero",
                  "frame": 100000
                },
                "gpu": false
              }
            }
          ]
        }
      }
    ],
    "env": [{
      "name": "CoinGame-v0",
      "max_t": null,
      "max_frame": 8000000,
      "num_envs": 1
    }],
    "body": {
      "product": "outer",
      "num": 1
    },
    "meta": {
      "distributed": false,
      "eval_frequency": 10000,
      "log_frequency": 10000,
      "max_session": 10,
      "max_concurrent_session":10,
      "max_trial": 1
    }
  },









  "coin_ppo_deploy_game_pm_le_self_play": {
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent": [
      {
        "name": "DeploymentGame",
        "memory": null,
        "net": null,
        "observing_other_agents": {
          "name": "FullyObservable"
        },
        "algorithm": {
          "name": "DeploymentGame",
          "redeploy_every_n_epi": 4000,
          "deploy_algo":{
            "name":"UCB1",
            "k_choice":2
          },
          "contained_algorithms": [
            {
              "name": "LE",
              "memory": null,
              "net": null,
              "welfare_function":"utilitarian_welfare",
              "observing_other_agents":{
                "name":"FullyObservable"
              },
              "algorithm": {
                "name": "LE",
                "punishement_time": 10,
                "min_coop_time": 0,
                "defection_detection_mode":"network_weights",
                "defection_carac_threshold": 0.01,
                "contained_algorithms": [
                  {
                    "name": "PPO",
                    "welfare_function":"utilitarian_welfare",
                    "observing_other_agents":{
                      "name":"FullyObservable"
                    },
                    "algorithm": {
                      "name": "PPO",
                      "action_pdtype": "default",
                      "action_policy": "default",
                      "explore_var_spec": null,
                      "gamma": 0.5,
                      "lam": 0.95,
                      "clip_eps_spec": {
                        "name": "linear_decay",
                        "start_val": 0.05,
                        "end_val": 0.005,
                        "start_step": 10000,
                        "end_step": 80000
                      },
                      "entropy_coef_spec": {
                        "name": "linear_decay",
                        "start_val": 0.3,
                        "end_val": 0.06,
                        "start_step": 0,
                        "end_step": 100000
                      },
                      "val_loss_coef": 1.0,
                      "time_horizon": 128,
                      "minibatch_size": 32,
                      "training_epoch": 8
                    },
                    "memory": {
                      "name": "OnPolicyBatchReplay"
                    },
                    "net": {
                      "type": "ConvNet",
                      "conv_hid_layers": [
                        [16, 3, 1, 1, 1],
                        [32, 3, 1, 0, 1]
                      ],
                      "shared": true,
                      "fc_hid_layers": [],
                      "hid_layers_activation": "relu",
                      "init_fn": "kaiming_normal_",
                      "normalize": true,
                      "batch_norm": false,
                      "clip_grad_val": 0.5,
                      "use_same_optim": false,
                      "actor_optim_spec": {
                        "name": "Adam",
                        "lr": 8e-4
                      },
                      "critic_optim_spec": {
                        "name": "Adam",
                        "lr": 8e-4
                      },
                      "lr_scheduler_spec": {
                        "name": "LinearToZero",
                        "frame": 100000
                      },
                      "gpu": false
                    }
                  },
                  {
                    "copy_n": 0
                  },
                  {
                    "copy_n": 0
                  }
                ]
              }
            }
          ]
        }
      },
      {
        "copy_n":0
      }
    ],
    "env": [{
      "name": "CoinGame-v0",
      "max_t": null,
      "max_frame": 8000000,
      "num_envs": 1
    }],
    "body": {
      "product": "outer",
      "num": 1
    },
    "meta": {
      "distributed": false,
      "eval_frequency": 10000,
      "log_frequency": 10000,
      "max_session": 1,
      "max_concurrent_session":10,
      "max_trial": 1
    }
  },


















  "coin_deploy_game_ppo_pm_le_with_naive_opponent": {
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent": [{
      "name": "DeploymentGame",
      "memory": null,
      "net": null,
      "observing_other_agents": {
        "name": "FullyObservable"
      },
      "algorithm": {
        "name": "DeploymentGame",
        "redeploy_every_n_epi": 4000,
        "deploy_algo":{
          "name":"UCB1",
          "k_choice":1
        },
        "contained_algorithms": [
          {
          "name": "LE",
          "memory": null,
          "net": null,
          "welfare_function":"utilitarian_welfare",
          "observing_other_agents":{
            "name":"FullyObservable"
          },
          "algorithm": {
            "name": "LE",
            "punishement_time": 10,
            "min_coop_time": 0,
            "defection_detection_mode":"network_weights",
            "defection_carac_threshold": 0.01,
            "contained_algorithms": [
              {
                "name": "PPO",
                "welfare_function":"utilitarian_welfare",
                "observing_other_agents":{
                  "name":"FullyObservable"
                },
                "algorithm": {
                  "name": "PPO",
                  "action_pdtype": "default",
                  "action_policy": "default",
                  "explore_var_spec": null,
                  "gamma": 0.5,
                  "lam": 0.95,
                  "clip_eps_spec": {
                    "name": "linear_decay",
                    "start_val": 0.05,
                    "end_val": 0.005,
                    "start_step": 10000,
                    "end_step": 80000
                  },
                  "entropy_coef_spec": {
                    "name": "linear_decay",
                    "start_val": 0.3,
                    "end_val": 0.06,
                    "start_step": 0,
                    "end_step": 100000
                  },
                  "val_loss_coef": 1.0,
                  "time_horizon": 128,
                  "minibatch_size": 32,
                  "training_epoch": 8
                },
                "memory": {
                  "name": "OnPolicyBatchReplay"
                },
                "net": {
                  "type": "ConvNet",
                  "conv_hid_layers": [
                    [16, 3, 1, 1, 1],
                    [32, 3, 1, 0, 1]
                  ],
                  "shared": true,
                  "fc_hid_layers": [16],
                  "hid_layers_activation": "relu",
                  "init_fn": "orthogonal_",
                  "normalize": false,
                  "batch_norm": false,
                  "clip_grad_val": 0.5,
                  "use_same_optim": false,
                  "actor_optim_spec": {
                    "name": "Adam",
                    "lr": 8e-4
                  },
                  "critic_optim_spec": {
                    "name": "Adam",
                    "lr": 8e-4
                  },
                  "lr_scheduler_spec": {
                    "name": "LinearToZero",
                    "frame": 100000
                  },
                  "gpu": false
                }
              },
              {
                "copy_n": 0
              },
              {
                "copy_n": 0
              }
            ]
          }
        }
        ]
      }
    },
    {
      "name": "DeploymentGame",
      "memory": null,
      "net": null,
      "observing_other_agents": {
        "name": "FullyObservable"
      },
      "algorithm": {
        "name": "DeploymentGame",
        "redeploy_every_n_epi": 4000,
        "deploy_algo":{
          "name":"UCB1",
          "k_choice":2
        },
        "contained_algorithms": [
          {
            "name": "PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{"name":"FullyObservable"},
            "algorithm": {
              "name": "PPO",
              "action_pdtype": "default",
              "action_policy": "default",
              "explore_var_spec": null,
              "gamma": 0.5,
              "lam": 0.95,
              "clip_eps_spec": {
                "name": "linear_decay",
                "start_val": 0.05,
                "end_val": 0.005,
                "start_step": 10000,
                "end_step": 80000
              },
              "entropy_coef_spec": {
                "name": "linear_decay",
                "start_val": 0.3,
                "end_val": 0.06,
                "start_step": 0,
                "end_step": 100000
              },
              "val_loss_coef": 1.0,
              "time_horizon": 128,
              "minibatch_size": 32,
              "training_epoch": 8
            },
            "memory": {
              "name": "OnPolicyBatchReplay"
            },
            "net": {
              "type": "ConvNet",
              "conv_hid_layers": [
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared": true,
              "fc_hid_layers": [16],
              "hid_layers_activation": "relu",
              "init_fn": "orthogonal_",
              "normalize": false,
              "batch_norm": false,
              "clip_grad_val": 0.5,
              "use_same_optim": false,
              "actor_optim_spec": {
                "name": "Adam",
                "lr": 8e-4
              },
              "critic_optim_spec": {
                "name": "Adam",
                "lr": 8e-4
              },
              "lr_scheduler_spec": {
                "name": "LinearToZero",
                "frame": 100000
              },
              "gpu": false
            }
          }
        ]
      }
    }
    ],
    "env": [{
      "name": "CoinGame-v0",
      "max_t": null,
      "max_frame": 8000000,
      "num_envs": 1
    }],
    "body": {
      "product": "outer",
      "num": 1
    },
    "meta": {
      "distributed": false,
      "eval_frequency": 10000,
      "log_frequency": 10000,
      "max_session": 10,
      "max_concurrent_session":10,
      "max_trial": 1
    }
  },







  "coin_deploy_game_ppo_nopm_spl_le_with_naive_opponent": {
    "world":{
      "name":"DefaultMultiAgentWorld",
      "deterministic":false
    },
    "agent": [{
      "name": "DeploymentGame",
      "memory": null,
      "net": null,
      "observing_other_agents": {
        "name": "FullyObservable"
      },
      "algorithm": {
        "name": "DeploymentGame",
        "redeploy_every_n_epi": 4000,
        "deploy_algo":{
          "name":"UCB1",
          "k_choice":1
        },
        "contained_algorithms": [
          {
          "name": "LE",
          "memory": null,
          "net": null,
          "welfare_function":"utilitarian_welfare",
          "observing_other_agents":{
            "name":"FullyObservable"
          },
          "algorithm": {
            "name": "LE",
            "punishement_time": 1,
            "min_coop_time": 0,
            "defection_detection_mode":"spl_observed_actions",
            "defection_carac_threshold": 0.0,
            "contained_algorithms": [
              {
                "name": "PPO",
                "welfare_function":"utilitarian_welfare",
                "observing_other_agents":{
                  "name":"FullyObservable"
                },
                "algorithm": {
                  "name": "PPO",
                  "action_pdtype": "default",
                  "action_policy": "default",
                  "explore_var_spec": null,
                  "gamma": 0.5,
                  "lam": 0.95,
                  "clip_eps_spec": {
                    "name": "linear_decay",
                    "start_val": 0.05,
                    "end_val": 0.005,
                    "start_step": 10000,
                    "end_step": 80000
                  },
                  "entropy_coef_spec": {
                    "name": "linear_decay",
                    "start_val": 0.3,
                    "end_val": 0.06,
                    "start_step": 0,
                    "end_step": 100000
                  },
                  "val_loss_coef": 1.0,
                  "time_horizon": 128,
                  "minibatch_size": 32,
                  "training_epoch": 8
                },
                "memory": {
                  "name": "OnPolicyBatchReplay"
                },
                "net": {
                  "type": "ConvNet",
                  "conv_hid_layers": [
                    [16, 3, 1, 1, 1],
                    [32, 3, 1, 0, 1]
                  ],
                  "shared": true,
                  "fc_hid_layers": [16],
                  "hid_layers_activation": "relu",
                  "init_fn": "orthogonal_",
                  "normalize": false,
                  "batch_norm": false,
                  "clip_grad_val": 0.5,
                  "use_same_optim": false,
                  "actor_optim_spec": {
                    "name": "Adam",
                    "lr": 8e-4
                  },
                  "critic_optim_spec": {
                    "name": "Adam",
                    "lr": 8e-4
                  },
                  "lr_scheduler_spec": {
                    "name": "LinearToZero",
                    "frame": 100000
                  },
                  "gpu": false
                }
              },
              {
                "copy_n": 0
              },
              {
                "copy_n": 0
              },
              {
                "name": "SupervisedLAPolicy",
                "algorithm": {
                  "name": "SupervisedLAPolicy",
                  "action_pdtype": "default",
                  "action_policy": "default",
                  "explore_var_spec": null,
                  "entropy_coef_spec": {
                    "name": "linear_decay",
                    "start_val": 0.0003,
                    "end_val": 0.00006,
                    "start_step": 0,
                    "end_step": 100000
                  },
                  "training_frequency": 24
                },
                "memory": {
                  "name": "OnPolicyBatchReplay"
                },
                "net": {
                  "type": "ConvNet",
                  "conv_hid_layers": [
                    [16, 3, 1,1,1],
                    [32,3,1,0,1]
                  ],
                  "shared": true,
                  "fc_hid_layers": [
                    16
                  ],
                  "hid_layers_activation": "relu",
                  "init_fn": "orthogonal_",
                  "normalize": false,
                  "batch_norm": false,
                  "clip_grad_val": 0.5,
                  "use_same_optim": false,
                  "loss_spec": {
                    "name": "MSELoss"
                  },
                  "actor_optim_spec": {
                    "name": "Adam",
                    "lr": 8e-4
                  },
                  "critic_optim_spec": {
                    "name": "Adam",
                    "lr": 8e-4
                  },
                  "lr_scheduler_spec": {
                    "name": "LinearToZero",
                    "frame": 100000
                  },
                  "gpu": false
                }
              }
            ]
          }
        }
        ]
      }
    },
    {
      "name": "DeploymentGame",
      "memory": null,
      "net": null,
      "observing_other_agents": {
        "name": "FullyObservable"
      },
      "algorithm": {
        "name": "DeploymentGame",
        "redeploy_every_n_epi": 4000,
        "deploy_algo":{
          "name":"UCB1",
          "k_choice":2
        },
        "contained_algorithms": [
          {
            "name": "PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{"name":"FullyObservable"},
            "algorithm": {
              "name": "PPO",
              "action_pdtype": "default",
              "action_policy": "default",
              "explore_var_spec": null,
              "gamma": 0.5,
              "lam": 0.95,
              "clip_eps_spec": {
                "name": "linear_decay",
                "start_val": 0.05,
                "end_val": 0.005,
                "start_step": 10000,
                "end_step": 80000
              },
              "entropy_coef_spec": {
                "name": "linear_decay",
                "start_val": 0.3,
                "end_val": 0.06,
                "start_step": 0,
                "end_step": 100000
              },
              "val_loss_coef": 1.0,
              "time_horizon": 128,
              "minibatch_size": 32,
              "training_epoch": 8
            },
            "memory": {
              "name": "OnPolicyBatchReplay"
            },
            "net": {
              "type": "ConvNet",
              "conv_hid_layers": [
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared": true,
              "fc_hid_layers": [16],
              "hid_layers_activation": "relu",
              "init_fn": "orthogonal_",
              "normalize": false,
              "batch_norm": false,
              "clip_grad_val": 0.5,
              "use_same_optim": false,
              "actor_optim_spec": {
                "name": "Adam",
                "lr": 8e-4
              },
              "critic_optim_spec": {
                "name": "Adam",
                "lr": 8e-4
              },
              "lr_scheduler_spec": {
                "name": "LinearToZero",
                "frame": 100000
              },
              "gpu": false
            }
          }
        ]
      }
    }
    ],
    "env": [{
      "name": "CoinGame-v0",
      "max_t": null,
      "max_frame": 8000000,
      "num_envs": 1
    }],
    "body": {
      "product": "outer",
      "num": 1
    },
    "meta": {
      "distributed": false,
      "eval_frequency": 10000,
      "log_frequency": 10000,
      "max_session": 10,
      "max_concurrent_session":10,
      "max_trial": 1
    }
  }







}
