{

  "coin_base":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"PPO",
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"PPO",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.5,
        "lam":0.5,
        "clip_eps_spec":{
          "name":"linear_decay",
          "start_val":0.05,
          "end_val":0.005,
          "start_step":10000,
          "end_step":160000
        },
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":0.3,
          "end_val":0.06,
          "start_step":0,
          "end_step":200000
        },
        "val_loss_coef":1.0,
        "time_horizon":128,
        "minibatch_size":32,
        "normalize_inputs":true,
        "training_epoch":8
      },
      "memory":{
        "name":"OnPolicyBatchReplay"
      },
      "net":{
        "type":"ConvNet",
        "conv_hid_layers":[
          [16, 3, 1, 1, 1],
          [32, 3, 1, 0, 1]
        ],
        "shared":true,
        "fc_hid_layers":[],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"kaiming_uniform_",
        "normalize":false,
        "batch_norm":false,
        "clip_grad_val":0.5,
        "use_same_optim":false,
        "actor_optim_spec":{
          "name":"SGD",
          "lr":0.008,
          "momentum":0.9
        },
        "critic_optim_spec":{
          "name":"SGD",
          "lr":0.008,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":200000
        },
        "gpu":false
      }
    },
    {
      "copy_n":0
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_base_util":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"PPO",
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"PPO",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.5,
        "lam":0.5,
        "clip_eps_spec":{
          "name":"linear_decay",
          "start_val":0.05,
          "end_val":0.005,
          "start_step":10000,
          "end_step":160000
        },
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":0.3,
          "end_val":0.06,
          "start_step":0,
          "end_step":200000
        },
        "val_loss_coef":1.0,
        "time_horizon":128,
        "minibatch_size":32,
        "normalize_inputs":true,
        "training_epoch":8
      },
      "memory":{
        "name":"OnPolicyBatchReplay"
      },
      "net":{
        "type":"ConvNet",
        "conv_hid_layers":[
          [16, 3, 1, 1, 1],
          [32, 3, 1, 0, 1]
        ],
        "shared":true,
        "fc_hid_layers":[],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"kaiming_uniform_",
        "normalize":false,
        "batch_norm":false,
        "clip_grad_val":0.5,
        "use_same_optim":false,
        "actor_optim_spec":{
          "name":"SGD",
          "lr":0.008,
          "momentum":0.9
        },
        "critic_optim_spec":{
          "name":"SGD",
          "lr":0.008,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":200000
        },
        "gpu":false
      }
    },
    {
      "copy_n":0
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },

  "coin_ppm_le_self_play":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":true},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"network_weights",
        "defection_carac_threshold":0.01,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          }
        ]
      }
    },
    {
      "copy_n":0
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppm_le_vs_naive_opp":{
    "world":{"name":"DefaultMultiAgentWorld",
             "deterministic":true},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"network_weights",
        "defection_carac_threshold":0.01,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          }
        ]
      }
    },
    {
      "name":"PPO",
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"PPO",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.5,
        "lam":0.5,
        "clip_eps_spec":{
          "name":"linear_decay",
          "start_val":0.05,
          "end_val":0.005,
          "start_step":10000,
          "end_step":160000
        },
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":0.3,
          "end_val":0.06,
          "start_step":0,
          "end_step":200000
        },
        "val_loss_coef":1.0,
        "time_horizon":128,
        "minibatch_size":32,
        "normalize_inputs":true,
        "training_epoch":8
      },
      "memory":{
        "name":"OnPolicyBatchReplay"
      },
      "net":{
        "type":"ConvNet",
        "conv_hid_layers":[
          [16, 3, 1, 1, 1],
          [32, 3, 1, 0, 1]
        ],
        "shared":true,
        "fc_hid_layers":[],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"kaiming_uniform_",
        "normalize":false,
        "batch_norm":false,
        "clip_grad_val":0.5,
        "use_same_optim":false,
        "actor_optim_spec":{
          "name":"SGD",
          "lr":0.008,
          "momentum":0.9
        },
        "critic_optim_spec":{
          "name":"SGD",
          "lr":0.008,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":200000
        },
        "gpu":false
      }
    }
    ],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },


  "coin_ppo_ipm_le_self_play_strat_5_lr_0008":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "copy_n":0
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_self_play_strat_5_lr_0002":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
      {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.002,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.002,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.02,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    }
    ],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_self_play_strat_5_lr_0032":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
      {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.032,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.032,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.32,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    }
    ],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_vs_naive_opp_strat_5_lr_0008":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"PPO",
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"PPO",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.5,
        "lam":0.5,
        "clip_eps_spec":{
          "name":"linear_decay",
          "start_val":0.05,
          "end_val":0.005,
          "start_step":10000,
          "end_step":160000
        },
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":0.3,
          "end_val":0.06,
          "start_step":0,
          "end_step":200000
        },
        "val_loss_coef":1.0,
        "time_horizon":128,
        "minibatch_size":32,
        "normalize_inputs":true,
        "training_epoch":8
      },
      "memory":{
        "name":"OnPolicyBatchReplay"
      },
      "net":{
        "type":"ConvNet",
        "conv_hid_layers":[
          [16, 3, 1, 1, 1],
          [32, 3, 1, 0, 1]
        ],
        "shared":true,
        "fc_hid_layers":[],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"kaiming_uniform_",
        "normalize":false,
        "batch_norm":false,
        "clip_grad_val":0.5,
        "use_same_optim":false,
        "actor_optim_spec":{
          "name":"SGD",
          "lr":0.008,
          "momentum":0.9
        },
        "critic_optim_spec":{
          "name":"SGD",
          "lr":0.008,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":200000
        },
        "gpu":false
      }
    }
    ],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_vs_naive_opp_strat_5_lr_0002":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"PPO",
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"PPO",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.5,
        "lam":0.5,
        "clip_eps_spec":{
          "name":"linear_decay",
          "start_val":0.05,
          "end_val":0.005,
          "start_step":10000,
          "end_step":160000
        },
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":0.3,
          "end_val":0.06,
          "start_step":0,
          "end_step":200000
        },
        "val_loss_coef":1.0,
        "time_horizon":128,
        "minibatch_size":32,
        "normalize_inputs":true,
        "training_epoch":8
      },
      "memory":{
        "name":"OnPolicyBatchReplay"
      },
      "net":{
        "type":"ConvNet",
        "conv_hid_layers":[
          [16, 3, 1, 1, 1],
          [32, 3, 1, 0, 1]
        ],
        "shared":true,
        "fc_hid_layers":[],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"kaiming_uniform_",
        "normalize":false,
        "batch_norm":false,
        "clip_grad_val":0.5,
        "use_same_optim":false,
        "actor_optim_spec":{
          "name":"SGD",
          "lr":0.002,
          "momentum":0.9
        },
        "critic_optim_spec":{
          "name":"SGD",
          "lr":0.002,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":200000
        },
        "gpu":false
      }
    }
    ],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_vs_naive_opp_strat_5_lr_0032":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"PPO",
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"PPO",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.5,
        "lam":0.5,
        "clip_eps_spec":{
          "name":"linear_decay",
          "start_val":0.05,
          "end_val":0.005,
          "start_step":10000,
          "end_step":160000
        },
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":0.3,
          "end_val":0.06,
          "start_step":0,
          "end_step":200000
        },
        "val_loss_coef":1.0,
        "time_horizon":128,
        "minibatch_size":32,
        "normalize_inputs":true,
        "training_epoch":8
      },
      "memory":{
        "name":"OnPolicyBatchReplay"
      },
      "net":{
        "type":"ConvNet",
        "conv_hid_layers":[
          [16, 3, 1, 1, 1],
          [32, 3, 1, 0, 1]
        ],
        "shared":true,
        "fc_hid_layers":[],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"kaiming_uniform_",
        "normalize":false,
        "batch_norm":false,
        "clip_grad_val":0.5,
        "use_same_optim":false,
        "actor_optim_spec":{
          "name":"SGD",
          "lr":0.032,
          "momentum":0.9
        },
        "critic_optim_spec":{
          "name":"SGD",
          "lr":0.032,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":200000
        },
        "gpu":false
      }
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },


  "coin_ppo_ipm_le_self_play_strat_2_lr_0008":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "copy_n":0
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_self_play_strat_2_lr_0002":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
      {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.002,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.002,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.02,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    }
    ],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_self_play_strat_2_lr_0032":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
      {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.032,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.032,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.32,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    }
    ],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_vs_naive_opp_strat_2_lr_0008":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"PPO",
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"PPO",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.5,
        "lam":0.5,
        "clip_eps_spec":{
          "name":"linear_decay",
          "start_val":0.05,
          "end_val":0.005,
          "start_step":10000,
          "end_step":160000
        },
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":0.3,
          "end_val":0.06,
          "start_step":0,
          "end_step":200000
        },
        "val_loss_coef":1.0,
        "time_horizon":128,
        "minibatch_size":32,
        "normalize_inputs":true,
        "training_epoch":8
      },
      "memory":{
        "name":"OnPolicyBatchReplay"
      },
      "net":{
        "type":"ConvNet",
        "conv_hid_layers":[
          [16, 3, 1, 1, 1],
          [32, 3, 1, 0, 1]
        ],
        "shared":true,
        "fc_hid_layers":[],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"kaiming_uniform_",
        "normalize":false,
        "batch_norm":false,
        "clip_grad_val":0.5,
        "use_same_optim":false,
        "actor_optim_spec":{
          "name":"SGD",
          "lr":0.008,
          "momentum":0.9
        },
        "critic_optim_spec":{
          "name":"SGD",
          "lr":0.008,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":200000
        },
        "gpu":false
      }
    }
    ],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_vs_naive_opp_strat_2_lr_0002":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"PPO",
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"PPO",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.5,
        "lam":0.5,
        "clip_eps_spec":{
          "name":"linear_decay",
          "start_val":0.05,
          "end_val":0.005,
          "start_step":10000,
          "end_step":160000
        },
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":0.3,
          "end_val":0.06,
          "start_step":0,
          "end_step":200000
        },
        "val_loss_coef":1.0,
        "time_horizon":128,
        "minibatch_size":32,
        "normalize_inputs":true,
        "training_epoch":8
      },
      "memory":{
        "name":"OnPolicyBatchReplay"
      },
      "net":{
        "type":"ConvNet",
        "conv_hid_layers":[
          [16, 3, 1, 1, 1],
          [32, 3, 1, 0, 1]
        ],
        "shared":true,
        "fc_hid_layers":[],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"kaiming_uniform_",
        "normalize":false,
        "batch_norm":false,
        "clip_grad_val":0.5,
        "use_same_optim":false,
        "actor_optim_spec":{
          "name":"SGD",
          "lr":0.002,
          "momentum":0.9
        },
        "critic_optim_spec":{
          "name":"SGD",
          "lr":0.002,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":200000
        },
        "gpu":false
      }
    }
    ],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_vs_naive_opp_strat_2_lr_0032":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"PPO",
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"PPO",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.5,
        "lam":0.5,
        "clip_eps_spec":{
          "name":"linear_decay",
          "start_val":0.05,
          "end_val":0.005,
          "start_step":10000,
          "end_step":160000
        },
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":0.3,
          "end_val":0.06,
          "start_step":0,
          "end_step":200000
        },
        "val_loss_coef":1.0,
        "time_horizon":128,
        "minibatch_size":32,
        "normalize_inputs":true,
        "training_epoch":8
      },
      "memory":{
        "name":"OnPolicyBatchReplay"
      },
      "net":{
        "type":"ConvNet",
        "conv_hid_layers":[
          [16, 3, 1, 1, 1],
          [32, 3, 1, 0, 1]
        ],
        "shared":true,
        "fc_hid_layers":[],
        "hid_layers_activation":"LeakyReLU",
        "init_fn":"kaiming_uniform_",
        "normalize":false,
        "batch_norm":false,
        "clip_grad_val":0.5,
        "use_same_optim":false,
        "actor_optim_spec":{
          "name":"SGD",
          "lr":0.032,
          "momentum":0.9
        },
        "critic_optim_spec":{
          "name":"SGD",
          "lr":0.032,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":200000
        },
        "gpu":false
      }
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },


  "coin_ppo_ipm_le_self_play_no_strat_lr_0008":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "copy_n":0
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_self_play_no_strat_lr_0002":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.002,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.002,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.02,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_self_play_no_strat_lr_0032":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.032,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.032,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.32,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_vs_naive_opp_no_strat_lr_0008":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"PPO",
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"PPO",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.5,
        "lam":0.5,
        "clip_eps_spec":{
          "name":"linear_decay",
          "start_val":0.05,
          "end_val":0.005,
          "start_step":10000,
          "end_step":160000
        },
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":0.3,
          "end_val":0.06,
          "start_step":0,
          "end_step":200000
        },
        "val_loss_coef":1.0,
        "time_horizon":128,
        "minibatch_size":32,
        "normalize_inputs":true,
        "training_epoch":8
      },
      "memory":{
        "name":"OnPolicyBatchReplay"
      },
      "net":{
        "type":"ConvNet",
        "conv_hid_layers":[
          [16, 3, 1, 1, 1],
          [32, 3, 1, 0, 1]
        ],
        "shared":true,
        "fc_hid_layers":[],
        "hid_layers_activation":"LeakyRELU",
        "init_fn":"kaiming_uniform_",
        "normalize":false,
        "batch_norm":false,
        "clip_grad_val":0.5,
        "use_same_optim":false,
        "actor_optim_spec":{
          "name":"SGD",
          "lr":0.008,
          "momentum":0.9
        },
        "critic_optim_spec":{
          "name":"SGD",
          "lr":0.008,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":200000
        },
        "gpu":false
      }
    }
    ],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_vs_naive_opp_no_strat_lr_0002":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"PPO",
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"PPO",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.5,
        "lam":0.5,
        "clip_eps_spec":{
          "name":"linear_decay",
          "start_val":0.05,
          "end_val":0.005,
          "start_step":10000,
          "end_step":160000
        },
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":0.3,
          "end_val":0.06,
          "start_step":0,
          "end_step":200000
        },
        "val_loss_coef":1.0,
        "time_horizon":128,
        "minibatch_size":32,
        "normalize_inputs":true,
        "training_epoch":8
      },
      "memory":{
        "name":"OnPolicyBatchReplay"
      },
      "net":{
        "type":"ConvNet",
        "conv_hid_layers":[
          [16, 3, 1, 1, 1],
          [32, 3, 1, 0, 1]
        ],
        "shared":true,
        "fc_hid_layers":[],
        "hid_layers_activation":"LeakyRELU",
        "init_fn":"kaiming_uniform_",
        "normalize":false,
        "batch_norm":false,
        "clip_grad_val":0.5,
        "use_same_optim":false,
        "actor_optim_spec":{
          "name":"SGD",
          "lr":0.002,
          "momentum":0.9
        },
        "critic_optim_spec":{
          "name":"SGD",
          "lr":0.002,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":200000
        },
        "gpu":false
      }
    }
    ],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_vs_naive_opp_no_strat_lr_0032":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"PPO",
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"PPO",
        "action_pdtype":"default",
        "action_policy":"default",
        "explore_var_spec":null,
        "gamma":0.5,
        "lam":0.5,
        "clip_eps_spec":{
          "name":"linear_decay",
          "start_val":0.05,
          "end_val":0.005,
          "start_step":10000,
          "end_step":160000
        },
        "entropy_coef_spec":{
          "name":"linear_decay",
          "start_val":0.3,
          "end_val":0.06,
          "start_step":0,
          "end_step":200000
        },
        "val_loss_coef":1.0,
        "time_horizon":128,
        "minibatch_size":32,
        "normalize_inputs":true,
        "training_epoch":8
      },
      "memory":{
        "name":"OnPolicyBatchReplay"
      },
      "net":{
        "type":"ConvNet",
        "conv_hid_layers":[
          [16, 3, 1, 1, 1],
          [32, 3, 1, 0, 1]
        ],
        "shared":true,
        "fc_hid_layers":[],
        "hid_layers_activation":"LeakyRELU",
        "init_fn":"kaiming_uniform_",
        "normalize":false,
        "batch_norm":false,
        "clip_grad_val":0.5,
        "use_same_optim":false,
        "actor_optim_spec":{
          "name":"SGD",
          "lr":0.032,
          "momentum":0.9
        },
        "critic_optim_spec":{
          "name":"SGD",
          "lr":0.032,
          "momentum":0.9
        },
        "lr_scheduler_spec":{
          "name":"LinearToZero",
          "frame":200000
        },
        "gpu":false
      }
    }
    ],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },


  "coin_ppo_ipm_le_vs_exploiter_no_strat_lr_0008":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"LEExploiter",
      "memory":null,
      "net":null,
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LEExploiter",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_vs_exploiter_strat_5_lr_0008":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"LEExploiter",
      "memory":null,
      "net":null,
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LEExploiter",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":true,
        "strat_5_coeff":10.0,
        "use_strat_2":false,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  },
  "coin_ppo_ipm_le_vs_exploiter_strat_2_lr_0008":{
    "world":{"name":"DefaultMultiAgentWorld"},
    "agent":[{
      "name":"LE",
      "memory":null,
      "net":null,
      "welfare_function":"utilitarian_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LE",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "welfare_function":"utilitarian_welfare",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    },
    {
      "name":"LEExploiter",
      "memory":null,
      "net":null,
      "welfare_function":"default_welfare",
      "observing_other_agents":{
        "name":"FullyObservable"
      },
      "algorithm":{
        "name":"LEExploiter",
        "punishement_time":1,
        "min_coop_time":0,
        "defection_detection_mode":"spl_observed_actions",
        "defection_carac_threshold":0.0,
        "average_d_carac":true,
        "average_d_carac_len":20,
        "coop_net_ent_diff_as_lr":false,
        "use_sl_for_simu_coop":false,
        "use_strat_4":false,
        "use_strat_5":false,
        "strat_5_coeff":10.0,
        "use_strat_2":true,
        "block_len":7,
        "meta_algo_memory":{
          "name":"Replay",
          "batch_size":1,
          "max_size":-1
        },
        "meta_algo_loss":{
          "name":"CrossEntropyLoss"
        },
        "contained_algorithms":[
          {
            "name":"PPO",
            "observing_other_agents":{
              "name":"FullyObservable"
            },
            "algorithm":{
              "name":"PPO",
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "gamma":0.5,
              "lam":0.5,
              "clip_eps_spec":{
                "name":"linear_decay",
                "start_val":0.05,
                "end_val":0.005,
                "start_step":10000,
                "end_step":160000
              },
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.3,
                "end_val":0.06,
                "start_step":0,
                "end_step":200000
              },
              "val_loss_coef":1.0,
              "time_horizon":128,
              "minibatch_size":32,
              "normalize_inputs":true,
              "training_epoch":8
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "shared":true,
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "clip_grad_val":0.5,
              "use_same_optim":false,
              "actor_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "critic_optim_spec":{
                "name":"SGD",
                "lr":0.008,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          },
          {
            "copy_n":0
          },
          {
            "copy_n":0
          },
          {
            "name":"SupervisedLAPolicy",
            "algorithm":{
              "name":"SupervisedLAPolicy",
              "normalize_inputs":true,
              "action_pdtype":"default",
              "action_policy":"default",
              "explore_var_spec":null,
              "entropy_coef_spec":{
                "name":"linear_decay",
                "start_val":0.0,
                "end_val":0.0,
                "start_step":0,
                "end_step":160000
              },
              "training_frequency":72
            },
            "memory":{
              "name":"OnPolicyBatchReplay"
            },
            "net":{
              "type":"ConvNet",
              "conv_hid_layers":[
                [16, 3, 1, 1, 1],
                [32, 3, 1, 0, 1]
              ],
              "fc_hid_layers":[],
              "hid_layers_activation":"LeakyReLU",
              "init_fn":"kaiming_uniform_",
              "normalize":false,
              "batch_norm":false,
              "loss_spec":{
                "name":"CrossEntropyLoss"
              },
              "optim_spec":{
                "name":"SGD",
                "lr":0.08,
                "momentum":0.9
              },
              "lr_scheduler_spec":{
                "name":"LinearToZero",
                "frame":200000
              },
              "gpu":false
            }
          }
        ]
      }
    }],
    "env":[{
      "name":"CoinGame-v0",
      "max_t":null,
      "max_frame":200000,
      "num_envs":1
    }],
    "body":{
      "product":"outer",
      "num":1
    },
    "meta":{
      "distributed":false,
      "eval_frequency":400,
      "log_frequency":400,
      "max_session":10,
      "max_concurrent_session":10,
      "max_trial":1
    }
  }

}