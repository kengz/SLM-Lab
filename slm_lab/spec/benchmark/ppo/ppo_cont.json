{
  "ppo_bipedalwalker": {
    "agent": {
      "name": "PPO",
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.99,
        "lam": 0.95,
        "clip_eps_spec": {
          "name": "no_decay",
          "start_val": 0.2,
          "end_val": 0.0,
          "start_step": 10000,
          "end_step": 1000000
        },
        "entropy_coef_spec": {
          "name": "no_decay",
          "start_val": 0.01,
          "end_val": 0.01,
          "start_step": 0,
          "end_step": 0
        },
        "val_loss_coef": 0.5,
        "time_horizon": 512,
        "minibatch_size": 4096,
        "training_epoch": 15
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          256,
          128
        ],
        "hid_layers_activation": "relu",
        "init_fn": "orthogonal_",
        "normalize": true,
        "batch_norm": false,
        "clip_grad_val": 0.5,
        "use_same_optim": true,
        "loss_spec": {
          "name": "MSELoss"
        },
        "actor_optim_spec": {
          "name": "AdamW",
          "lr": 0.0003
        },
        "critic_optim_spec": {
          "name": "AdamW",
          "lr": 0.0003
        },
        "gpu": "auto"
      }
    },
    "env": {
      "name": "BipedalWalker-v3",
      "num_envs": 32,
      "max_t": null,
      "max_frame": 4000000.0
    },
    "meta": {
      "distributed": false,
      "log_frequency": 1600,
      "eval_frequency": 10000,
      "max_session": 1,
      "max_trial": 50,
      "search_scheduler": {
        "grace_period": 200000,
        "reduction_factor": 3
      }
    },
    "search": {
      "agent": {
        "algorithm": {
          "gamma__uniform": [
            0.98,
            0.999
          ],
          "lam__uniform": [
            0.92,
            0.98
          ],
          "clip_eps_spec": {
            "start_val__uniform": [
              0.1,
              0.3
            ]
          },
          "entropy_coef_spec": {
            "start_val__loguniform": [
              0.0001,
              0.01
            ]
          },
          "val_loss_coef__uniform": [
            0.2,
            2.0
          ],
          "training_epoch__choice": [
            10,
            15,
            20
          ]
        },
        "net": {
          "hid_layers__choice": [
            [
              256,
              128
            ],
            [
              256,
              256
            ],
            [
              512,
              256
            ]
          ],
          "actor_optim_spec": {
            "lr__loguniform": [
              0.0001,
              0.001
            ]
          },
          "critic_optim_spec": {
            "lr__loguniform": [
              0.0001,
              0.001
            ]
          }
        }
      }
    }
  },
  "ppo_pendulum": {
    "agent": {
      "name": "PPO",
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.99,
        "lam": 0.95,
        "clip_eps_spec": {
          "name": "no_decay",
          "start_val": 0.2,
          "end_val": 0.0,
          "start_step": 10000,
          "end_step": 1000000
        },
        "entropy_coef_spec": {
          "name": "no_decay",
          "start_val": 0.01,
          "end_val": 0.01,
          "start_step": 0,
          "end_step": 0
        },
        "val_loss_coef": 0.5,
        "time_horizon": 512,
        "minibatch_size": 4096,
        "training_epoch": 15
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          256,
          128
        ],
        "hid_layers_activation": "relu",
        "init_fn": "orthogonal_",
        "normalize": true,
        "batch_norm": false,
        "clip_grad_val": 0.5,
        "use_same_optim": true,
        "loss_spec": {
          "name": "MSELoss"
        },
        "actor_optim_spec": {
          "name": "AdamW",
          "lr": 0.0003
        },
        "critic_optim_spec": {
          "name": "AdamW",
          "lr": 0.0003
        },
        "gpu": "auto"
      }
    },
    "env": {
      "name": "Pendulum-v1",
      "num_envs": 32,
      "max_t": null,
      "max_frame": 1000000.0
    },
    "meta": {
      "distributed": false,
      "log_frequency": 10000,
      "eval_frequency": 10000,
      "max_session": 1,
      "max_trial": 30,
      "search_scheduler": {
        "grace_period": 50000,
        "reduction_factor": 3
      }
    },
    "search": {
      "agent": {
        "algorithm": {
          "gamma__uniform": [
            0.95,
            0.999
          ],
          "lam__uniform": [
            0.9,
            0.98
          ],
          "clip_eps_spec": {
            "start_val__uniform": [
              0.1,
              0.3
            ]
          },
          "entropy_coef_spec": {
            "start_val__loguniform": [
              0.0001,
              0.01
            ]
          },
          "val_loss_coef__uniform": [
            0.2,
            2.0
          ],
          "training_epoch__choice": [
            10,
            15,
            20
          ]
        },
        "net": {
          "hid_layers__choice": [
            [
              128,
              64
            ],
            [
              256,
              128
            ],
            [
              128,
              128
            ]
          ],
          "actor_optim_spec": {
            "lr__loguniform": [
              0.0001,
              0.002
            ]
          },
          "critic_optim_spec": {
            "lr__loguniform": [
              0.0001,
              0.002
            ]
          }
        }
      }
    }
  },
  "ppo_bipedalwalker_iter1": {
    "agent": {
      "name": "PPO",
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.99,
        "lam": 0.95,
        "clip_eps_spec": {
          "name": "linear_decay",
          "start_val": 0.2,
          "end_val": 0.1,
          "start_step": 0,
          "end_step": 2000000
        },
        "entropy_coef_spec": {
          "name": "linear_decay",
          "start_val": 0.01,
          "end_val": 0.001,
          "start_step": 0,
          "end_step": 2000000
        },
        "val_loss_coef": 0.5,
        "time_horizon": 2048,
        "minibatch_size": 256,
        "training_epoch": 10
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          256,
          256
        ],
        "hid_layers_activation": "tanh",
        "init_fn": "orthogonal_",
        "clip_grad_val": 1.0,
        "use_same_optim": false,
        "loss_spec": {
          "name": "MSELoss"
        },
        "actor_optim_spec": {
          "name": "Adam",
          "lr": 0.0003
        },
        "critic_optim_spec": {
          "name": "Adam",
          "lr": 0.001
        },
        "gpu": "auto"
      }
    },
    "env": {
      "name": "BipedalWalker-v3",
      "num_envs": 16,
      "max_t": null,
      "max_frame": 2000000.0
    },
    "meta": {
      "distributed": false,
      "log_frequency": 10000,
      "eval_frequency": 10000,
      "max_session": 1,
      "max_trial": 1
    }
  },
  "ppo_pendulum_iter1": {
    "agent": {
      "name": "PPO",
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.99,
        "lam": 0.95,
        "clip_eps_spec": {
          "name": "linear_decay",
          "start_val": 0.2,
          "end_val": 0.1,
          "start_step": 0,
          "end_step": 500000
        },
        "entropy_coef_spec": {
          "name": "linear_decay",
          "start_val": 0.01,
          "end_val": 0.001,
          "start_step": 0,
          "end_step": 500000
        },
        "val_loss_coef": 0.5,
        "time_horizon": 1024,
        "minibatch_size": 128,
        "training_epoch": 10
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          128,
          128
        ],
        "hid_layers_activation": "tanh",
        "init_fn": "orthogonal_",
        "clip_grad_val": 1.0,
        "use_same_optim": false,
        "loss_spec": {
          "name": "MSELoss"
        },
        "actor_optim_spec": {
          "name": "Adam",
          "lr": 0.0005
        },
        "critic_optim_spec": {
          "name": "Adam",
          "lr": 0.002
        },
        "gpu": "auto"
      }
    },
    "env": {
      "name": "Pendulum-v1",
      "num_envs": 8,
      "max_t": null,
      "max_frame": 500000
    },
    "meta": {
      "distributed": false,
      "log_frequency": 5000,
      "eval_frequency": 5000,
      "max_session": 1,
      "max_trial": 1
    }
  },
  "ppo_pendulum_iter2": {
    "agent": {
      "name": "PPO",
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.99,
        "lam": 0.95,
        "clip_eps_spec": {
          "name": "linear_decay",
          "start_val": 0.2,
          "end_val": 0.1,
          "start_step": 0,
          "end_step": 500000
        },
        "entropy_coef_spec": {
          "name": "linear_decay",
          "start_val": 0.005,
          "end_val": 0.001,
          "start_step": 0,
          "end_step": 500000
        },
        "val_loss_coef": 0.5,
        "time_horizon": 1024,
        "minibatch_size": 128,
        "training_epoch": 10
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          128,
          128
        ],
        "hid_layers_activation": "tanh",
        "init_fn": "orthogonal_",
        "normalize": true,
        "clip_grad_val": 0.5,
        "use_same_optim": false,
        "loss_spec": {
          "name": "MSELoss"
        },
        "actor_optim_spec": {
          "name": "Adam",
          "lr": 0.0001
        },
        "critic_optim_spec": {
          "name": "Adam",
          "lr": 0.0005
        },
        "gpu": "auto"
      }
    },
    "env": {
      "name": "Pendulum-v1",
      "num_envs": 8,
      "max_t": null,
      "max_frame": 500000
    },
    "meta": {
      "distributed": false,
      "log_frequency": 5000,
      "eval_frequency": 5000,
      "max_session": 1,
      "max_trial": 1
    }
  },
  "ppo_bipedalwalker_iter2": {
    "agent": {
      "name": "PPO",
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.99,
        "lam": 0.95,
        "clip_eps_spec": {
          "name": "linear_decay",
          "start_val": 0.2,
          "end_val": 0.1,
          "start_step": 0,
          "end_step": 2000000
        },
        "entropy_coef_spec": {
          "name": "linear_decay",
          "start_val": 0.005,
          "end_val": 0.001,
          "start_step": 0,
          "end_step": 2000000
        },
        "val_loss_coef": 0.5,
        "time_horizon": 2048,
        "minibatch_size": 256,
        "training_epoch": 10
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          256,
          256
        ],
        "hid_layers_activation": "tanh",
        "init_fn": "orthogonal_",
        "normalize": true,
        "clip_grad_val": 0.5,
        "use_same_optim": false,
        "loss_spec": {
          "name": "MSELoss"
        },
        "actor_optim_spec": {
          "name": "Adam",
          "lr": 0.0001
        },
        "critic_optim_spec": {
          "name": "Adam",
          "lr": 0.0005
        },
        "gpu": "auto"
      }
    },
    "env": {
      "name": "BipedalWalker-v3",
      "num_envs": 16,
      "max_t": null,
      "max_frame": 2000000.0
    },
    "meta": {
      "distributed": false,
      "log_frequency": 10000,
      "eval_frequency": 10000,
      "max_session": 1,
      "max_trial": 1
    }
  },
  "ppo_pendulum_iter3": {
    "agent": {
      "name": "PPO",
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.99,
        "lam": 0.95,
        "clip_eps_spec": {
          "name": "no_decay",
          "start_val": 0.1,
          "end_val": 0.1,
          "start_step": 0,
          "end_step": 0
        },
        "entropy_coef_spec": {
          "name": "no_decay",
          "start_val": 0.001,
          "end_val": 0.001,
          "start_step": 0,
          "end_step": 0
        },
        "val_loss_coef": 0.5,
        "time_horizon": 512,
        "minibatch_size": 64,
        "training_epoch": 4
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          64,
          64
        ],
        "hid_layers_activation": "tanh",
        "init_fn": "orthogonal_",
        "clip_grad_val": 0.5,
        "use_same_optim": false,
        "loss_spec": {
          "name": "MSELoss"
        },
        "actor_optim_spec": {
          "name": "Adam",
          "lr": 1e-05
        },
        "critic_optim_spec": {
          "name": "Adam",
          "lr": 0.0001
        },
        "gpu": "auto"
      }
    },
    "env": {
      "name": "Pendulum-v1",
      "num_envs": 4,
      "max_t": null,
      "max_frame": 500000
    },
    "meta": {
      "distributed": false,
      "log_frequency": 5000,
      "eval_frequency": 5000,
      "max_session": 1,
      "max_trial": 1
    }
  },
  "ppo_bipedalwalker_asha_best": {
    "agent": {
      "name": "PPO",
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.9805,
        "lam": 0.9644,
        "clip_eps_spec": {
          "name": "no_decay",
          "start_val": 0.1633,
          "end_val": 0.1633,
          "start_step": 0,
          "end_step": 0
        },
        "entropy_coef_spec": {
          "name": "no_decay",
          "start_val": 0.0065,
          "end_val": 0.0065,
          "start_step": 0,
          "end_step": 0
        },
        "val_loss_coef": 1.47,
        "time_horizon": 2048,
        "minibatch_size": 256,
        "training_epoch": 20
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          256,
          128
        ],
        "hid_layers_activation": "tanh",
        "init_fn": "orthogonal_",
        "clip_grad_val": 1.0,
        "use_same_optim": false,
        "loss_spec": {
          "name": "MSELoss"
        },
        "actor_optim_spec": {
          "name": "Adam",
          "lr": 0.00042435
        },
        "critic_optim_spec": {
          "name": "Adam",
          "lr": 0.00011458
        },
        "gpu": "auto"
      }
    },
    "env": {
      "name": "BipedalWalker-v3",
      "num_envs": 16,
      "max_t": null,
      "max_frame": 3000000.0
    },
    "meta": {
      "distributed": false,
      "log_frequency": 10000,
      "eval_frequency": 10000,
      "max_session": 1,
      "max_trial": 1
    }
  },
  "ppo_pendulum_asha_best": {
    "agent": {
      "name": "PPO",
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.972,
        "lam": 0.9745,
        "clip_eps_spec": {
          "name": "no_decay",
          "start_val": 0.1624,
          "end_val": 0.1624,
          "start_step": 0,
          "end_step": 0
        },
        "entropy_coef_spec": {
          "name": "no_decay",
          "start_val": 0.004,
          "end_val": 0.004,
          "start_step": 0,
          "end_step": 0
        },
        "val_loss_coef": 0.93,
        "time_horizon": 1024,
        "minibatch_size": 128,
        "training_epoch": 10
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          256,
          128
        ],
        "hid_layers_activation": "tanh",
        "init_fn": "orthogonal_",
        "clip_grad_val": 0.5,
        "use_same_optim": false,
        "loss_spec": {
          "name": "MSELoss"
        },
        "actor_optim_spec": {
          "name": "Adam",
          "lr": 0.000318
        },
        "critic_optim_spec": {
          "name": "Adam",
          "lr": 0.001134
        },
        "gpu": "auto"
      }
    },
    "env": {
      "name": "Pendulum-v1",
      "num_envs": 8,
      "max_t": null,
      "max_frame": 1000000.0
    },
    "meta": {
      "distributed": false,
      "log_frequency": 10000,
      "eval_frequency": 10000,
      "max_session": 1,
      "max_trial": 1
    }
  },
  "ppo_bipedalwalker_stage2_search": {
    "agent": {
      "name": "PPO",
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.98,
        "lam": 0.96,
        "clip_eps_spec": {
          "name": "no_decay",
          "start_val": 0.16,
          "end_val": 0.16,
          "start_step": 0,
          "end_step": 0
        },
        "entropy_coef_spec": {
          "name": "no_decay",
          "start_val": 0.006,
          "end_val": 0.006,
          "start_step": 0,
          "end_step": 0
        },
        "val_loss_coef": 1.4,
        "time_horizon": 2048,
        "minibatch_size": 256,
        "training_epoch": 20
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          256,
          128
        ],
        "hid_layers_activation": "tanh",
        "init_fn": "orthogonal_",
        "clip_grad_val": 1.0,
        "use_same_optim": false,
        "loss_spec": {
          "name": "MSELoss"
        },
        "actor_optim_spec": {
          "name": "Adam",
          "lr": 0.0004
        },
        "critic_optim_spec": {
          "name": "Adam",
          "lr": 0.0001
        },
        "gpu": "auto"
      }
    },
    "env": {
      "name": "BipedalWalker-v3",
      "num_envs": 16,
      "max_t": null,
      "max_frame": 2000000.0
    },
    "meta": {
      "distributed": false,
      "log_frequency": 10000,
      "eval_frequency": 10000,
      "max_session": 4,
      "max_trial": 10
    },
    "search": {
      "agent.algorithm.gamma__choice": [
        0.975,
        0.98,
        0.985
      ],
      "agent.algorithm.lam__choice": [
        0.955,
        0.964,
        0.975
      ],
      "agent.algorithm.clip_eps_spec.start_val__choice": [
        0.15,
        0.163,
        0.18
      ],
      "agent.algorithm.entropy_coef_spec.start_val__choice": [
        0.005,
        0.0065,
        0.008
      ],
      "agent.algorithm.val_loss_coef__choice": [
        1.3,
        1.47,
        1.6
      ],
      "agent.algorithm.training_epoch__choice": [
        18,
        20,
        22
      ],
      "agent.net.hid_layers__choice": [
        [
          256,
          128
        ],
        [
          256,
          256
        ],
        [
          384,
          192
        ]
      ],
      "agent.net.actor_optim_spec.lr__choice": [
        0.0003,
        0.000424,
        0.0005
      ],
      "agent.net.critic_optim_spec.lr__choice": [
        8e-05,
        0.000115,
        0.00015
      ]
    }
  },
  "ppo_bipedalwalker_refined_asha": {
    "agent": {
      "name": "PPO",
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.99,
        "lam": 0.95,
        "clip_eps_spec": {
          "name": "no_decay",
          "start_val": 0.2,
          "end_val": 0.2,
          "start_step": 0,
          "end_step": 0
        },
        "entropy_coef_spec": {
          "name": "no_decay",
          "start_val": 0.01,
          "end_val": 0.01,
          "start_step": 0,
          "end_step": 0
        },
        "val_loss_coef": 0.5,
        "time_horizon": 2048,
        "minibatch_size": 64,
        "training_epoch": 10
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          64,
          64
        ],
        "hid_layers_activation": "tanh",
        "init_fn": "orthogonal_",
        "clip_grad_val": 0.5,
        "use_same_optim": false,
        "loss_spec": {
          "name": "MSELoss"
        },
        "actor_optim_spec": {
          "name": "Adam",
          "lr": 0.0003
        },
        "critic_optim_spec": {
          "name": "Adam",
          "lr": 0.001
        },
        "gpu": "auto"
      }
    },
    "env": {
      "name": "BipedalWalker-v3",
      "num_envs": 16,
      "max_t": null,
      "max_frame": 3000000.0
    },
    "meta": {
      "distributed": false,
      "log_frequency": 10000,
      "eval_frequency": 10000,
      "max_session": 1,
      "max_trial": 25,
      "search_scheduler": {
        "grace_period": 300000,
        "reduction_factor": 3
      }
    },
    "search": {
      "agent.algorithm.gamma__uniform": [
        0.95,
        0.995
      ],
      "agent.algorithm.lam__uniform": [
        0.9,
        0.98
      ],
      "agent.algorithm.clip_eps_spec.start_val__uniform": [
        0.1,
        0.3
      ],
      "agent.algorithm.entropy_coef_spec.start_val__loguniform": [
        0.001,
        0.02
      ],
      "agent.algorithm.val_loss_coef__uniform": [
        0.3,
        2.0
      ],
      "agent.algorithm.training_epoch__choice": [
        10,
        15,
        20
      ],
      "agent.algorithm.minibatch_size__choice": [
        64,
        128,
        256
      ],
      "agent.net.hid_layers__choice": [
        [
          64,
          64
        ],
        [
          128,
          128
        ],
        [
          256,
          128
        ],
        [
          256,
          256
        ]
      ],
      "agent.net.clip_grad_val__choice": [
        0.5,
        1.0,
        2.0
      ],
      "agent.net.actor_optim_spec.lr__loguniform": [
        5e-05,
        0.001
      ],
      "agent.net.critic_optim_spec.lr__loguniform": [
        5e-05,
        0.002
      ]
    }
  },
  "ppo_mountaincar_continuous": {
    "agent": {
      "name": "PPO",
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.99,
        "lam": 0.95,
        "clip_eps_spec": {
          "name": "linear_decay",
          "start_val": 0.2,
          "end_val": 0.1,
          "start_step": 0,
          "end_step": 500000
        },
        "entropy_coef_spec": {
          "name": "linear_decay",
          "start_val": 0.01,
          "end_val": 0.001,
          "start_step": 0,
          "end_step": 500000
        },
        "val_loss_coef": 0.5,
        "time_horizon": 1024,
        "minibatch_size": 128,
        "training_epoch": 10
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          128,
          128
        ],
        "hid_layers_activation": "tanh",
        "init_fn": "orthogonal_",
        "clip_grad_val": 1.0,
        "use_same_optim": false,
        "loss_spec": {
          "name": "MSELoss"
        },
        "actor_optim_spec": {
          "name": "Adam",
          "lr": 0.0005
        },
        "critic_optim_spec": {
          "name": "Adam",
          "lr": 0.002
        },
        "gpu": "auto"
      }
    },
    "env": {
      "name": "MountainCarContinuous-v0",
      "num_envs": 8,
      "max_t": null,
      "max_frame": 500000
    },
    "meta": {
      "distributed": false,
      "log_frequency": 5000,
      "eval_frequency": 5000,
      "max_session": 4,
      "max_trial": 1
    }
  }
}
