# SLM-Lab: Modular Deep Reinforcement Learning Framework

## Project Overview

Modular deep reinforcement learning framework in PyTorch. Originally designed for comprehensive RL experimentation with flexible algorithm implementations and environment support. Currently being migrated to modern dependencies (gymnasium, latest PyTorch, etc.).

## Development Environment

### Package Management

- **Always use `uv`** instead of pip/python
- Installation: `uv add package-name`
- Running scripts: `uv run script.py`

### Cloud Compute

- **Use dstack** for GPU-intensive training and development
- Setup: Follow [dstack documentation](https://dstack.ai/docs/)
- Run: `dstack apply -f .dstack/workflows/<file>.yml`

## Code Standards

### Style Guide

- **Naming**: Concise and globally consistent for searchability
  - Use clear, searchable names: `train_ppo()` not `tp()`
  - Consistent patterns: `*_config`, `*_wrapper`, `*_callback`
- **Type Hints**: Native Python types preferred, keep simple
  - `list[str]` over `List[str]`
  - `dict[str, float]` over `Dict[str, float]`
  - Simple unions: `str | None` over `Optional[str]`
- **Code Complexity**: Prioritize readability and brevity
  - Short functions (max ~20 lines)
  - Avoid nested complexity - extract to helper functions
  - Clear variable names over comments when possible
- **Refactoring**: Be obsessively clean
  - Refactor immediately when code feels complex
  - Remove dead code aggressively
  - Simplify whenever possible - shorter is usually better

### SLM-Lab Best Practices

- Use gymnasium (not gym) for all environments
- Follow new gymnasium API: `reset()` returns `(obs, info)`, `step()` returns `(obs, reward, terminated, truncated, info)`
- Handle both discrete and continuous action spaces
- Use vectorized environments for parallel training
- Maintain modular algorithm structure with separate agent/network/memory components

## Version Management

- **Versioning**: Follow semantic versioning (SemVer)
- **Commits**: Use Angular commit convention (`feat:`, `fix:`, `docs:`, etc.)

## Notes for Claude Code Assistant

When working on this project:

1. **Always suggest `uv` commands** instead of pip/python
2. **Use dstack** for compute-intensive suggestions
3. **Expect project structure to evolve** - keep suggestions flexible
4. **Follow Angular commit convention** for all commits
5. **Use semantic versioning** for releases
6. **Use the TODO below** to organize work, check off when done
7. **When task is done, do quick tests, update TODO, update MIGRATION_CHANGELOG.md, then cleanup and commit**
8. **Refactor frequently, keep code DRY and simple**

## Framework Design Patterns

### SLM-Lab Architecture

SLM-Lab follows a modular design pattern with these core components:

1. **Agent** (`slm_lab/agent/`) - Algorithm implementations (A2C, PPO, SAC, etc.)
2. **Environment** (`slm_lab/env/`) - Environment wrappers and utilities
3. **Networks** (`slm_lab/agent/net/`) - Neural network architectures
4. **Memory** (`slm_lab/agent/memory/`) - Experience replay and storage
5. **Experiment** (`slm_lab/experiment/`) - Training loop and search utilities
6. **Spec System** (`slm_lab/spec/`) - JSON configuration for reproducible experiments

### Key Components

- **Environment wrappers**: Support for OpenAI/gymnasium, Unity, VizDoom
- **Algorithm diversity**: DQN, A2C, PPO, SAC, and variants
- **Network types**: MLP, ConvNet, RNN with flexible architectures
- **Memory systems**: Experience replay, prioritized replay
- **Experiment management**: Hyperparameter search, distributed training

### Migration Status

Currently migrating from:

- `gym` â†’ `gymnasium` (new API with `terminated`/`truncated`)
- `roboschool` â†’ removed (deprecated)
- Atari environments â†’ `ale-py` under gymnasium
- PyTorch updates â†’ modern optimizers and schedulers

## How to Run SLM-Lab

```bash
# Command structure
uv run run_lab.py {spec_file} {spec_name} {lab_mode}

# Lab modes:
#   dev     - Development mode with verbose logging, env rendering, gradient checks (slower but helpful)
#   train   - Training mode for fastest performance (disables dev tools)
#   train@{predir} - Resume training from specific directory (e.g. train@latest)
#   enjoy@{session_spec_file} - Replay trained model from trial-session
#   search  - Hyperparameter search (uses Ray - avoid until stable)

# Examples:
uv run run_lab.py slm_lab/spec/demo.json dqn_cartpole dev
uv run run_lab.py slm_lab/spec/benchmark/dqn/dqn_cartpole.json vanilla_dqn_boltzmann_cartpole train
uv run run_lab.py slm_lab/spec/benchmark/ppo/ppo_cartpole.json ppo_shared_cartpole dev

# Spec files are located in slm_lab/spec/ with structure:
# {
#   "spec_name": {
#     "agent": [{...}],
#     "env": [{...}],
#     "search": [{...}]
#   }
# }

# Representative Environment Tests (run after env-related changes):
# NOTE run them with timeout of 30s if you are testing instead of waiting for completion that can take a long time.

# 1. Single discrete:
uv run run_lab.py slm_lab/spec/benchmark/reinforce/reinforce_cartpole.json reinforce_cartpole train

# 2. Vector discrete:
uv run run_lab.py slm_lab/spec/benchmark/dqn/ddqn_per_lunar.json ddqn_per_concat_lunar train

# 3. Single continuous:
uv run run_lab.py slm_lab/spec/cont_test.json a2c_gae_pendulum_single train
# action dim = 4
uv run run_lab.py slm_lab/spec/cont_test.json a2c_gae_bipedalwalker_single train

# 4. Vector continuous:
uv run run_lab.py slm_lab/spec/cont_test.json a2c_gae_pendulum train
# action dim = 4
uv run run_lab.py slm_lab/spec/cont_test.json a2c_gae_bipedalwalker train

# 5. Atari check:
uv run run_lab.py slm_lab/spec/benchmark/a2c/a2c_gae_pong.json a2c_gae_pong train

# 6. MuJoCo environments check:
uv run run_lab.py slm_lab/spec/benchmark/sac/sac_mujoco.json sac_mujoco train

# Representative Benchmark Commands (for full training runs):

# Discrete Environments:
uv run run_lab.py slm_lab/spec/benchmark/dqn/dqn_lunar.json dqn_concat_lunar train
uv run run_lab.py slm_lab/spec/benchmark/ppo/ppo_lunar.json ppo_lunar train
uv run run_lab.py slm_lab/spec/benchmark/sac/sac_lunar.json sac_lunar train
uv run run_lab.py slm_lab/spec/benchmark/dqn/dqn_pong.json dqn_pong train
uv run run_lab.py slm_lab/spec/benchmark/ppo/ppo_pong.json ppo_pong train

# Continuous Environments (MuJoCo):
uv run run_lab.py slm_lab/spec/benchmark/a2c/a2c_gae_mujoco.json a2c_gae_mujoco train
uv run run_lab.py slm_lab/spec/benchmark/a2c/a2c_nstep_mujoco.json a2c_nstep_mujoco train
uv run run_lab.py slm_lab/spec/benchmark/ppo/ppo_mujoco.json ppo_mujoco train
uv run run_lab.py slm_lab/spec/benchmark/sac/sac_mujoco.json sac_mujoco train
uv run run_lab.py slm_lab/spec/benchmark/async_sac/async_sac_mujoco.json async_sac_mujoco train

# Humanoid (Longer Training):
uv run run_lab.py slm_lab/spec/benchmark/a2c/a2c_gae_mujoco.json a2c_gae_humanoid train
uv run run_lab.py slm_lab/spec/benchmark/ppo/ppo_mujoco.json ppo_humanoid train
uv run run_lab.py slm_lab/spec/benchmark/sac/sac_mujoco.json sac_humanoid train
uv run run_lab.py slm_lab/spec/benchmark/async_sac/async_sac_mujoco.json async_sac_humanoid train
```

## TODO - Migration Progress

### âœ… Completed

- [x] **Core Framework Migration**: Updated to gymnasium API (terminated/truncated), modern PyTorch
- [x] **Environment Updates**: CartPole-v0 â†’ v1, removed deprecated roboschool dependencies
- [x] **Dependencies**: Updated to PyTorch 2.8.0, gymnasium, modern package versions
- [x] **Basic Testing**: DQN, PPO, A2C working on CartPole-v1
- [x] **Atari Environments**: Fixed ale-py duplicate registration, verified integration with gymnasium
- [x] **VecEnv Cleanup**: Fixed get_viewer with pygame fallback, improved render handling
- [x] **Render API**: Fixed modern gymnasium render() method compatibility
- [x] **Logger Migration**: Migrated to loguru with improved formatting and configurable metrics

### âœ… Recently Completed

- [x] **Code Cleanup**: Simplified `get_action_type()` in policy_util.py - removed fallback methods since only called from one place
- [x] **Environment Base Cleanup**: Consolidated lengthy `_get` methods in slm_lab/env/base.py - inlined dimension calculation logic
- [x] **Environment Gym Logic Review**: Cleaned up slm_lab/env/gym.py - removed sync fallback, simplified spec handling and action conversion
- [x] **Distribution Module Consolidation**: Removed duplicate slm_lab/lib/distribution_new.py - was identical to distribution.py
- [x] **Gymnasium Migration Verification**: Verified full compliance with gymnasium migration guide - correct API usage, render modes, seed handling
- [x] **SAC Algorithm Fixes**: Fixed target entropy calculation (-log(action_dim) instead of -action_dim) and removed vector environment awareness from algorithm
- [x] **Vector Environment Support**: Proper handling at env/agent level with clean separation of concerns - no openai.py file exists
- [x] **Testing Verification**: Confirmed DQN, PPO, A2C, SAC work with both single and vector environments

### âœ… Recently Completed

- [x] **Comprehensive Action Shape Handling**: Complete gymnasium compatibility for all action types

  - [x] **Universal Action Conversion**: Implemented `to_action()` method in `Algorithm` base class handling all 8 action type combinations
  - [x] **Discrete Actions**: Single â†’ scalar int, Vector â†’ (num_envs,) array
  - [x] **Continuous Actions**: Single â†’ (action_dim,), Vector â†’ (num_envs, action_dim)
  - [x] **Simplified Implementation**: Reduced from 31 lines to 15 lines while maintaining full functionality
  - [x] **Comprehensive Testing**: Created unit tests in `test/env/test_action_conversion.py` with 5 focused test cases
  - [x] **Full Verification**: All 8 combinations tested with real environments - CartPole, LunarLander, Pendulum, BipedalWalker

- [x] **SAC Algorithm Optimization**: Completed comprehensive improvements to Soft Actor-Critic

  - [x] **GumbelSoftmax Optimization**: Updated distribution.py to use PyTorch's efficient Gumbel noise generation pattern
  - [x] **Vector Environment Support**: Fixed policy_util.random() to properly handle vector environments during warmup
  - [x] **SAC act() Method Simplification**: Reduced from 14 to 6 lines, unified with other algorithms using standard squeeze() approach
  - [x] **Target Entropy Improvement**: Implemented epsilon-greedy policy bounds for principled discrete SAC target entropy calculation
  - [x] **Backward Compatibility**: Set epsilon=0.1 as reasonable default (10% exploration) with proper documentation

- [x] **Vector Environment Performance Optimization**: Fixed performance issues with smart vectorization mode selection

  - [x] **Problem Analysis**: Identified that always using async vectorization was suboptimal for small numbers of environments
  - [x] **Simple Rule Implementation**: Use sync for â‰¤8 environments, async for >8 environments (avoids fragile environment name matching)
  - [x] **Performance Benefit**: Sync avoids subprocess overhead for small env counts, async leverages parallelization for large counts
  - [x] **Testing Verified**: CartPole with 2 envs now uses sync vectorization achieving 1600-2000 fps without crashes

- [x] **Environment Wrapper Optimization**: Improved TrackReward wrapper following gymnasium conventions

  - [x] **Proper Reset Handling**: Reset total_reward to 0.0 at episode start and add to info dict for consistency
  - [x] **Clear Wrapper Usage**: Direct class instantiation following standard gymnasium wrapper conventions (no factory methods)
  - [x] **Vector Environment Optimization**: Improved vectorized operations using numpy array operations instead of loops
  - [x] **Code Cleanup**: Removed unnecessary complexity while maintaining SLM-Lab compatibility
  - [x] **Testing Verified**: Both single and vector environments work correctly with proper reward tracking

- [x] **Environment Testing Complete**: Comprehensive testing of all environment improvements

  - [x] **Single Environment**: TrackReward wrapper, proper reset/step functionality
  - [x] **Vector Environment**: VectorTrackReward wrapper, sync/async vectorization modes
  - [x] **Reward Tracking**: Proper accumulation and reset handling for both single and vector environments
  - [x] **Vectorization Logic**: Verified sync for â‰¤8 envs, async for >8 envs working correctly
  - [x] **Integration Testing**: All components working together seamlessly

- [x] **Roboschool Migration**: Migrated all roboschool environments to gymnasium MuJoCo v5 equivalents

  - [x] **Environment Mapping**: Mapped 8 core roboschool environments to gymnasium MuJoCo equivalents (Ant, HalfCheetah, Hopper, etc.)
  - [x] **Version Update**: Updated from v4 to v5 for latest gymnasium compatibility
  - [x] **MuJoCo Installation**: Added gymnasium[mujoco] dependency for physics simulation
  - [x] **Spec File Updates**: Updated all 6 roboschool benchmark spec files (SAC, PPO, A2C, async SAC)
  - [x] **Verification**: Tested A2C on Hopper-v5 with proper reward tracking and vectorization

### âœ… Recently Completed

- [x] **Script Cleanup**: Removed outdated package.json and unnecessary script complexity
  - [x] **Complete Removal**: Eliminated package.json entirely (was only used internally)
  - [x] **Native Integration**: Added typer CLI directly in `retro_analysis.py` (no separate files)
  - [x] **Clean pyproject.toml**: Only 2 entries: main `slm-lab` and `slm-retro` commands
  - [x] **Zero Clutter**: From 8+ npm scripts â†’ 1 native function with minimal CLI wrapper
  - [x] **Modern Approach**: Uses standard `uv run` commands for all development tasks
  - [x] **Essential Only**: Kept only `retro_analyze` function that actually exists and is used

### ðŸ“ Pending

- [ ] **RNN Functionality Restoration**: Fix RecurrentNet sequence input format issues
  - [ ] **Root Cause Analysis**: RNN tests fail because RecurrentNet expects sequence input but receives single state dimensions
  - [ ] **Proper Input Handling**: Implement correct input reshaping for RNN networks (batch_size, seq_len, input_dim)
  - [ ] **Test Data Format**: Ensure test environments provide proper sequential data for RNN algorithms
  - [ ] **Integration Testing**: Verify RNN variants work: DRQN, A2C-RNN, PPO-RNN, SIL-RNN across all environments
- [ ] work on env wrapper: TrackTime
- [x] **Performance Optimizations**: Modern hardware and PyTorch optimizations
  - [x] **torch.compile**: Auto-enabled on GPU, 20-30% speedup potential
  - [x] **Cached Tensors**: SAC action scaling moved to GPU tensors (eliminates numpyâ†’torch conversion)
  - [x] **Precision**: Reduced unnecessary float64â†’float32 conversions
  - [x] **Smart Vectorization**: Sync for â‰¤8 envs, async for >8 envs
  - [ ] **Benchmark Testing**: Measure actual speedup gains
  - [ ] **Higher Parallelization**: Test performance with more vector environments
- [ ] test atari working - just Pong is enough. I'll do the full run with dstack so give me cmd
- [ ] mark for future: new gymnasium environments https://farama.org/projects
- [x] **CPU-bound Optimizations**: Key hot paths optimized
  - [x] **SAC**: Eliminated repeated numpyâ†’torch conversions in action scaling
  - [x] **Policy Utils**: Smart dtype handling to avoid unnecessary copies  
  - [x] **Device Transfers**: Cached tensors on correct device
  - [ ] **Numba Integration**: Could explore for remaining numpy bottlenecks
- [ ] **Unit Tests**: Run full test suite when framework is stable
- [ ] **Ray Integration**: Fix hyperparameter search functionality (lowest priority). see https://docs.ray.io/en/latest/tune/index.html - now it also uses Optuna for smarter search than just random/grid search
- [ ] **Documentation**: Update gitbook documentation for new API changes
- [ ] **Performance Validation**: Ensure migrated algorithms achieve expected performance

### ðŸš« Removed/Deprecated

- [x] **roboschool**: Migrated to gymnasium MuJoCo (deprecated upstream)
- [x] **RoboschoolAtlasForwardWalk-v1**: Removed (no direct gymnasium equivalent)
- [x] **RoboschoolPong-v1**: Replaced with PongNoFrameskip-v4 (Atari equivalent)
- [x] **pybullet_envs**: Removed from imports (will use gymnasium equivalents)

### Command to Test Current State

```bash
# Basic functionality tests
uv run run_lab.py slm_lab/spec/demo.json dqn_cartpole dev
uv run run_lab.py slm_lab/spec/benchmark/ppo/ppo_cartpole.json ppo_shared_cartpole dev
uv run run_lab.py slm_lab/spec/benchmark/a2c/a2c_gae_cartpole.json a2c_gae_cartpole dev

# Retrospective analysis (the only custom CLI we kept)
uv run slm-retro data/experiment_dir

# Performance optimizations (automatic on GPU, manual on CPU)
TORCH_COMPILE=true uv run run_lab.py slm_lab/spec/benchmark/ppo/ppo_cartpole.json ppo_shared_cartpole train

# Test modern VectorEnv (4x performance improvement for vectorized environments) 
USE_MODERN_VECENV=true uv run run_lab.py slm_lab/spec/benchmark/dqn/dqn_cartpole.json vanilla_dqn_boltzmann_cartpole dev
```
