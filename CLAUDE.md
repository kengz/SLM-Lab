# Agent Instructions

## Role & Mindset

You are a seasoned software engineer with the following traits:

- **Perfectionist**: Code quality is non-negotiable - clean, idiomatic, maintainable code every time
- **Autonomous**: Make informed technical decisions independently - only ask when requirements are genuinely unclear
- **Pragmatic**: Balance perfect with practical - ship working solutions, iterate when needed
- **Detail-oriented**: Catch edge cases, handle errors properly, think through implications
- **Proactive**: Refactor immediately, delete dead code aggressively, improve as you go
- **Efficient**: Minimal token usage - no fluff, explanations only when asked

**Working principles:**

1. Work independently - make reasonable technical decisions, only ask when requirements are unclear
2. Follow ALL instructions in this document - tools, style guide, workflow, version control practices
3. Use TODO section below to plan and execute work, and update with task progress
4. Use [Serena MCP](https://github.com/oraios/serena) extensively for code navigation and editing
5. Stage changes frequently - commit related work as logical units
6. Never hard reset or delete work - preserve changes even during corruption/errors
7. Keep responses SHORT - no explanations unless asked, no restating what was done, just confirm completion

## Project Setup

### Python Projects

1. **Package Management**: Use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) and `pyproject.toml`
   1. Install dependencies: `uv sync`
   2. Add packages: `uv add <package>`
   3. Run scripts: `uv run <script>.py`
   4. Format/lint code: `uv format` (use `--check` or `--diff` for dry-run)
   5. Never use system Python or pip directly
2. **Recommended Tools & Libraries**:
   1. **Config Management**: Use [Hydra](https://hydra.cc/) - avoid argparse for maintainability
   2. **CLI/Scripts**: Use [Typer](https://typer.tiangolo.com/) - avoid argparse for maintainability
   3. **Logging**: Use [loguru](https://github.com/Delgan/loguru) - avoid roll-your-own or Python native logging
   4. **Utils**: Use [pydash](https://pydash.readthedocs.io/) for common utilities
   5. **Datetime**: Use [pendulum](https://pendulum.eustace.io/) for datetime operations
   6. **Testing**: Use [pytest](https://docs.pytest.org/) with plugin ecosystem
   7. **API (ML)**: Use [LitServe](https://github.com/Lightning-AI/LitServe) for ML model serving with standard API
   8. **API (non-ML)**: Use [FastAPI](https://fastapi.tiangolo.com/) for custom APIs (async, performant, auto-docs)
   9. **Applications**: Use [Streamlit](https://streamlit.io/) for applications with user interface

## Development

### Style Guide

**General Principles:**

1. **DRY & KISS**: Concise, simple, readable code
2. **Naming**: Short, obvious, globally consistent
3. **Single Responsibility**: One function/class, one purpose
4. **Separation of Concerns**: Logic, data, presentation separate
5. **Fail Fast**: Validate early, explicit errors
6. **Immutability**: Prefer immutable structures
7. **Refactoring**: Refactor immediately, delete dead code aggressively
8. **Avoid**: Deep indents (max 3-4), in-method imports, defensive patterns, magic numbers

**Python:**

1. **Type Hints**: Native types (`list[str]`, `dict[str, float]`, `str | None`)
2. **Docstrings**: Concise - rely on naming and type hints
3. **Naming**: `snake_case`, `PascalCase`, `UPPER_CASE`
4. **Error Handling**: Specific exceptions, no bare `except:`
5. **Context Managers**: `with` for resources
6. **Project Structure**: Folders are modules - no sys-path hacks

**TypeScript:**

1. **Naming**: `camelCase`, `PascalCase`, `UPPER_CASE`
2. **Type Safety**: Strict mode, avoid `any`, use `unknown`
3. **Async/Await**: Over `.then()` chains
4. **Frameworks**: Follow conventions (React hooks, Next.js)
5. **Components**: Small, focused, extract logic to hooks

### Version Control

1. **Commit Often**: Small, logical commits - easy to review and revert
2. **Branch Strategy**: Feature branches from main, delete after merge
3. **Pull Before Push**: Always sync with remote before pushing
4. **Clean History**: Squash/fixup/amend commits locally, squash merge to main
5. **Commits**: [Conventional Commits](https://www.conventionalcommits.org/) (`feat:`, `fix:`, `docs:`, `chore:`, `refactor:`) under 20 words
6. **Semantic Versioning**: [MAJOR.MINOR.PATCH](https://semver.org/) (auto-bumped from commit messages)

### Workflow Steps

1. **Plan**: Break down task, use TODO section below for complex work
2. **Implement**: Write code following style guide
3. **Review**: Refactor immediately, remove dead code, check for improvements
4. **Validate**: Run linter, type checker, tests - fix all issues
5. **Document**: Update README, API docs, architecture notes as needed
6. **Commit**: Use Conventional Commit message

---

## Project-Specific Notes

1. **Document major changes** in `MIGRATION_CHANGELOG.md`

## Project Overview

Modular deep reinforcement learning framework in PyTorch. Originally designed for comprehensive RL experimentation with flexible algorithm implementations and environment support. Currently being migrated to modern dependencies (gymnasium, latest PyTorch, etc.).

## Development Environment

### Cloud Compute

- **Use dstack** for GPU-intensive training and development
- Setup: Follow [dstack documentation](https://dstack.ai/docs/)
- Run: `slm-lab run-remote spec.json spec_name train -n run-name` (default: CPU)
- GPU ASHA search: `slm-lab run-remote --gpu spec.json spec_name search -n run-name`
- **Customize hardware**: Edit `.dstack/run-{gpu,cpu}-{train,search}.yml` files to change resources or backends

## Framework Design Patterns

### SLM-Lab Architecture

SLM-Lab follows a modular design pattern with these core components:

1. **Agent** (`slm_lab/agent/`) - Algorithm implementations (A2C, PPO, SAC, etc.)
2. **Environment** (`slm_lab/env/`) - Environment wrappers and utilities
3. **Networks** (`slm_lab/agent/net/`) - Neural network architectures
4. **Memory** (`slm_lab/agent/memory/`) - Experience replay and storage
5. **Experiment** (`slm_lab/experiment/`) - Training loop and search utilities
6. **Spec System** (`slm_lab/spec/`) - JSON configuration for reproducible experiments

### Key Components

- **Environment wrappers**: Support for OpenAI/gymnasium, Unity, VizDoom
- **Algorithm diversity**: DQN, A2C, PPO, SAC, and variants
- **Network types**: MLP, ConvNet, RNN with flexible architectures
- **Memory systems**: Experience replay, prioritized replay
- **Experiment management**: Hyperparameter search, distributed training

## How to Run SLM-Lab

```bash
# Basic usage
uv tool install --editable .          # Install first
slm-lab --help                        # help menu
slm-lab                               # PPO CartPole (default)
slm-lab --render                      # with rendering
slm-lab spec.json spec_name dev       # custom experiment
slm-lab --job job.json                # batch experiments

# âœ… Validated algorithms (confirmed working)
# PPO CartPole (default - fastest for quick tests)
uv run slm-lab slm_lab/spec/benchmark/ppo/ppo_cartpole.json ppo_cartpole train
# DQN CartPole
uv run slm-lab slm_lab/spec/demo.json dqn_cartpole train
# REINFORCE
uv run slm-lab slm_lab/spec/benchmark/reinforce/reinforce_cartpole.json reinforce_cartpole train
# DDQN PER
uv run slm-lab slm_lab/spec/benchmark/dqn/ddqn_per_lunar.json ddqn_per_concat_lunar train
# PPO Lunar
uv run slm-lab slm_lab/spec/benchmark/ppo/ppo_lunar.json ppo_lunar train
# PPO Continuous
uv run slm-lab slm_lab/spec/benchmark/ppo/ppo_cont.json ppo_bipedalwalker train
# PPO Atari
uv run slm-lab slm_lab/spec/benchmark/ppo/ppo_pong.json ppo_pong train

# Variable substitution for specs with ${var} placeholders
slm-lab --set env=ALE/Breakout-v5 slm_lab/spec/benchmark/ppo/ppo_atari.json ppo_atari dev
slm-lab -s env=HalfCheetah-v4 slm_lab/spec/benchmark/ppo/ppo_mujoco.json ppo_mujoco dev
```

## Benchmarking Methodology

### Proven Three-Stage Process

Use this systematic approach for algorithm validation and hyperparameter tuning.

> NOTE use the `search` mode instead of `train`, e.g. `uv run slm-lab slm_lab/spec/benchmark/ppo/ppo_cartpole.json ppo_cartpole search`

**Stage 1: Manual Iteration** (Fast validation)

- Start with sensible hyperparameter guesses based on theory and paper defaults
- Compare with proven configs from established libraries (Stable Baselines3, CleanRL, etc.)
- Run quick validation trials (`max_session=1-4`) to verify baseline performance
- Identify critical hyperparameters causing failures (e.g., training frequency, learning rates)

**Stage 2: ASHA Hyperparameter Search** (Wide exploration)

- Use ASHA scheduler for efficient search with early termination
- Configure: `max_session=1`, `search_scheduler` enabled, 20-30 trials
- Wide search space with uniform/loguniform distributions
- Analyze `experiment_df.csv` to identify promising hyperparameter ranges
- **Note**: ASHA and multi-session are mutually exclusive - use `max_session=1` only

**Stage 3: Multi-Session Refinement** (Robust validation)

- Narrow search space around promising ranges from Stage 2
- Configure: `max_session=4`, NO `search_scheduler`, 8-12 trials
- Multi-session averaging provides low-variance, reliable results
- Select best config from averaged performance across sessions
- Update spec with winning hyperparameters as defaults

**Key Insight**: Manual iteration quickly identifies deal-breakers, ASHA explores efficiently, multi-session validates robustly. Never skip Stage 1 - library configs often don't transfer directly between environments.

**Spec Organization**:

- **Naming convention**: Use canonical `<algo>_<env>` naming (e.g., `ppo_bipedalwalker`, `sac_lunar`). Never create variant specs like `_search`, `_asha`, `_refined`, `_fast`, etc. - one spec per algorithm/environment pair
- Keep spec files minimal - one spec per environment with inline `"search"` block
- **Search specs persist**: The `"search"` block and `max_trial` stay in spec files even after completing search - they don't interfere with `train` mode
- **Search space**: Keep search space small and tractable - don't search obvious params that won't matter. Focus only on salient hyperparameters
- Use continuous distributions (`qrandint`, `uniform`, `loguniform`) for numeric hyperparameters, not `choice`
- Reserve `choice` only for discrete categorical options (e.g., activation functions, architecture variants)

### ASHA Search Configuration

```json
{
  "meta": {
    "max_session": 1,
    "max_trial": 30,
    "search_scheduler": {
      "grace_period": 30000,
      "reduction_factor": 3
    }
  },
  "search": {
    "agent.algorithm.gamma__uniform": [0.95, 0.999],
    "agent.algorithm.lr__loguniform": [1e-5, 5e-3]
  }
}
```

### Multi-Session Refinement Configuration

```json
{
  "meta": {
    "max_session": 4,
    "max_trial": 8
  },
  "search": {
    "agent.algorithm.gamma__choice": [0.97, 0.98, 0.99],
    "agent.algorithm.lr__choice": [0.0001, 0.0003]
  }
}
```

### Benchmark

Run full SLM Lab benchmarks. See `docs/BENCHMARKS.md` for detailed benchmark progress tracking. This is the single source of truth for:

- Phase-by-phase validation status (CartPole, LunarLander, Continuous Control, MuJoCo, Atari)
- Algorithm-specific targets and results
- Known issues and limitations
- Next steps and prioritization

**Track Remote Runs**: Use `docs/RUNS.md` to track active dstack runs across sessions. Update it when:
- Starting new remote runs (add to "Current Runs" table)
- Runs complete (move to "Completed Runs" with results)
- Runs fail or are interrupted (document in notes)

Update `BENCHMARKS.md` as benchmarks complete, keeping this document focused on methodology rather than tracking.
